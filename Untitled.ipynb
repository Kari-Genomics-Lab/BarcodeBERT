{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07401629-af0e-4fb7-af70-aaefdb7cfb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "\n",
    "\"\"\"# Model\"\"\"\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, maxlen, device):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(int(vocab_size), int(d_model))  # token embedding\n",
    "        self.pos_embed = nn.Embedding(int(maxlen), int(d_model))  # position embedding\n",
    "        self.norm = nn.LayerNorm(int(d_model))\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=self.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos)\n",
    "        return self.norm(embedding)\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, device):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    pad_attn_mask = pad_attn_mask.to(device)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, n_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_k)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs,\n",
    "                                               enc_self_attn_mask)  # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)  # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.linear = nn.Linear(self.n_heads * self.d_v, self.d_model)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,\n",
    "                                                                                 2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,\n",
    "                                                                                 2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,\n",
    "                                                                                 2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1,\n",
    "                                                  1)  # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1,\n",
    "                                                            self.n_heads * self.d_v)  # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = self.linear(context)\n",
    "\n",
    "        return self.layernorm(output + residual), attn  # output: [batch_size x len_q x d_model]\n",
    "\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.l1 = nn.Linear(d_model, d_ff)\n",
    "        self.l2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.relu = GELU()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs\n",
    "        output = self.l1(inputs)\n",
    "        output = self.relu(output)\n",
    "        output = self.l2(output)\n",
    "        return self.layer_norm(output + residual)\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        self.d_k = d_k\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(\n",
    "            self.d_k)  # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9)  # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "class BERT_MLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, maxlen, n_layers, d_k, d_v, n_heads, device):\n",
    "        super(BERT_MLM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, maxlen, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, d_k, d_v, n_heads) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = GELU()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 10)\n",
    "        \n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        output = self.embedding(input_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            # embedding layer\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        h_masked = self.norm(self.activ2(self.linear(output)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "\n",
    "        return logits_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d0026a8c-a303-4392-8f01-ecdf4bd2c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 2, 3, 30]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class kmer_tokenizer(object):\n",
    "\n",
    "    def __init__(self, k, stride=1,  padding=False, max_len=66):\n",
    "        self.k = k\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, dna_sequence) -> list:\n",
    "        tokens=[]\n",
    "        if self.padding:\n",
    "            if len(dna_sequence) >  self.max_len:\n",
    "                x = dna_sequence[:self.max_len]\n",
    "            else:\n",
    "                x = dna_sequence + 'N'*(self.max_len-len(dna_sequence))\n",
    "\n",
    "        for i in range(0, len(x) - self.k + 1, self.stride):\n",
    "            k_mer = x[i:i+self.k]\n",
    "            tokens.append(k_mer)\n",
    "        return tokens\n",
    "\n",
    "\n",
    "\n",
    "k = 4\n",
    "max_len = 660\n",
    "\n",
    "\n",
    "tokenizer = kmer_tokenizer(k, stride=k, padding=True, max_len=max_len)  #Non overlapping k-mers\n",
    "\n",
    "\n",
    "kmer_iter = ([''.join(kmer)] for kmer in  product('ACGT',repeat=k))\n",
    "vocab = build_vocab_from_iterator(kmer_iter, specials=[\"<MASK>\",\"<CLS>\",\"<UNK>\"])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "print(vocab(['ACCG', 'NNGT', 'AAAA', 'ACGT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b6254940-9a80-4b99-a715-ae4dce06daea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the model . . .\n",
      "The model has been succesfully initialized . . .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb Cell 3\u001b[0m in \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m masked_input[mask_idx] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m masked_input \u001b[39m=\u001b[39m masked_input\u001b[39m.\u001b[39mview(batch\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m out \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mk\u001b[39m+\u001b[39m\u001b[39m3\u001b[39m), batch\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m#\u001b[39;00m\n",
      "File \u001b[0;32m~/dev_iDeLUCS/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb Cell 3\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(input_ids)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m     enc_self_attn_mask \u001b[39m=\u001b[39m get_attn_pad_mask(input_ids, input_ids, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m         \u001b[39m# embedding layer\u001b[39;00m\n",
      "File \u001b[0;32m~/dev_iDeLUCS/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb Cell 3\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m pos \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(seq_len, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m pos \u001b[39m=\u001b[39m pos\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mexpand_as(x)  \u001b[39m# (seq_len,) -> (batch_size, seq_len)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtok_embed(x) \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_embed(pos)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loan/Nextcloud/CodeRepos/Bumblebee/Untitled.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(embedding)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv(\"data/supervised.tsv\", sep='\\t')\n",
    "target_level='species_name'\n",
    "\n",
    "barcodes =  train['nucleotides'].to_list()\n",
    "\n",
    "def collate_function(batch):\n",
    "\n",
    "    features = []\n",
    "    \n",
    "    for _barcode in batch:\n",
    "        processed_barcode = torch.tensor(vocab(tokenizer(_barcode)), dtype=torch.int64)\n",
    "        features.append(processed_barcode)\n",
    "\n",
    "    features = torch.stack(features)\n",
    "\n",
    "    return  features.to(device)\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "config = {\n",
    "    \"d_model\": 768,\n",
    "    \"n_heads\": 2,\n",
    "    \"n_layers\": 2,\n",
    "    \"max_len\": 512\n",
    "\n",
    "}\n",
    "    \n",
    "print(\"Initializing the model . . .\")\n",
    "model = BERT_MLM(vocab_size, config[\"d_model\"], max_len, config[\"n_layers\"], 32, 32,\n",
    "             config[\"n_heads\"], device=device)\n",
    "print(\"The model has been succesfully initialized . . .\")\n",
    "\n",
    "model.train()\n",
    "epochs = 1\n",
    "\n",
    "total_loss = 0 \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1e-2, total_iters=20)\n",
    "\n",
    "dataloader = DataLoader(barcodes, batch_size=5, shuffle=True, collate_fn=collate_function)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        #Build the masking on the fly\n",
    "        masked_input = batch.clone()\n",
    "        random_mask = torch.rand(masked_input.shape) # I can only do this for non-overlapping\n",
    "        random_mask = (random_mask < 0.25) * (masked_input != 2) #Cannot mask the [<UNK>] token\n",
    "        mask_idx=(random_mask.flatten() == True).nonzero().view(-1)\n",
    "        masked_input = masked_input.flatten()\n",
    "        masked_input[mask_idx] = 1\n",
    "        masked_input = masked_input.view(batch.size())\n",
    "\n",
    "        out = model(batch)\n",
    "        loss = criterion(out.view(-1,4**k+3), batch.view(-1))\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    if (epoch+1)%40==0 or epoch==0:\n",
    "        print(\"Epoch: {} -> loss: {}\".format(epoch+1, total_loss/(len(dataloader)*epoch+1)))\n",
    "        torch.save(model.state_dict(), 'model_checkpoints/pre_trained_model_full.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bad4e8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:,1:,:].mean(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6082c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.activ1(model.fc(out[:, 0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4b4b0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(77.4386, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "print(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
