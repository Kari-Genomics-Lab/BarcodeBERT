{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e52a8b2-6c93-46a1-93e7-ac8158ad4fd3",
   "metadata": {},
   "source": [
    "# Testing DNABERT embedding \n",
    "\n",
    "Here we will test if we can use the pre-trained DNABERt model to embedd DNA barcodes into a \"valid metric space\". Mainly, we want to contrast the results of 1-kNN accuracy with the 1D-CNN architecture and stablish a baseline for our model trained from scratch. To my understaning, these are the parameters of the model:\n",
    "\n",
    "* Sequence Length: 512\n",
    "* Backbone: BERT_small\n",
    "* Proportion of the input masked: 15%\n",
    "* Hidden Layer Size: 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70600d59-4666-43da-a509-60b5183b80cb",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2193a148-85a6-46dd-812d-96594dc07823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#Read dataset file\n",
    "dataset = pd.read_csv('../barcode_insect_database.tsv',sep='\\t')\n",
    "\n",
    "# Start filetring to get high quality Unseen sequences\n",
    "# In particular: Sequences from known species with more than 50 individuals per species.\n",
    "filtered = dataset[dataset['species_name'].notna()]\n",
    "s = filtered.groupby('species_name').sampleid.count()\n",
    "l = s[s > 50].index.to_list()\n",
    "\n",
    "#Sample 100 species at random to be unseen by our model for generalization.\n",
    "selected_species = random.sample(l, k=100)\n",
    "\n",
    "print(len(set(selected_species)))\n",
    "unseen = filtered[filtered['species_name'].isin(selected_species)]\n",
    "train = pd.concat([dataset, unseen, unseen]).drop_duplicates(keep=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805bab1-0051-494b-a7c9-dcb106c5873b",
   "metadata": {},
   "source": [
    "## Testing DNABERT out-of-the-box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eeba1a-896f-46fa-9d6a-7964a6340f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the libraries\n",
    "import sys\n",
    "sys.path.append('/home/pmillana/projects/def-khill22/pmillana/DNABERT/src')   #This is wher I saved the transformers library binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185d8253-d302-4b36-a893-69653666db12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoModelForMaskedLM' from 'transformers' (/home/pmillana/projects/def-khill22/pmillana/DNABERT/src/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_161034/218383554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zhihan1996/DNA_bert_6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zhihan1996/DNA_bert_6\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoModelForMaskedLM' from 'transformers' (/home/pmillana/projects/def-khill22/pmillana/DNABERT/src/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "def predict(args, model, tokenizer, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    pred_task_names = (args.task_name,)\n",
    "    pred_outputs_dirs = (args.predict_dir,)\n",
    "    if not os.path.exists(args.predict_dir):\n",
    "        os.makedirs(args.predict_dir)\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    predictions = {}\n",
    "    for pred_task, pred_output_dir in zip(pred_task_names, pred_outputs_dirs):\n",
    "        pred_dataset = load_and_cache_examples(args, pred_task, tokenizer, evaluate=True)\n",
    "\n",
    "        if not os.path.exists(pred_output_dir) and args.local_rank in [-1, 0]:\n",
    "            os.makedirs(pred_output_dir)\n",
    "\n",
    "        args.pred_batch_size = args.per_gpu_pred_batch_size * max(1, args.n_gpu)\n",
    "        # Note that DistributedSampler samples randomly\n",
    "        pred_sampler = SequentialSampler(pred_dataset)\n",
    "        pred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size=args.pred_batch_size)\n",
    "\n",
    "        # multi-gpu eval\n",
    "        if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running prediction {} *****\".format(prefix))\n",
    "        logger.info(\"  Num examples = %d\", len(pred_dataset))\n",
    "        logger.info(\"  Batch size = %d\", args.pred_batch_size)\n",
    "        pred_loss = 0.0\n",
    "        nb_pred_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "        for batch in tqdm(pred_dataloader, desc=\"Predicting\"):\n",
    "            model.eval()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "                if args.model_type != \"distilbert\":\n",
    "                    inputs[\"token_type_ids\"] = (\n",
    "                        batch[2] if args.model_type in TOKEN_ID_GROUP else None\n",
    "                    )  \n",
    "                outputs = model(**inputs)\n",
    "                _, logits, hidden_state = outputs[:3]\n",
    "                print(hidden_state[0].shape)\n",
    "\n",
    "            if preds is None:\n",
    "                preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "                \n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        if args.output_mode == \"classification\":\n",
    "            if args.task_name[:3] == \"dna\" and args.task_name != \"dnasplice\":\n",
    "                if args.do_ensemble_pred:\n",
    "                    probs = softmax(torch.tensor(preds, dtype=torch.float32)).numpy()\n",
    "                else:\n",
    "                    probs = softmax(torch.tensor(preds, dtype=torch.float32))[:,1].numpy()\n",
    "            elif args.task_name == \"dnasplice\":\n",
    "                probs = softmax(torch.tensor(preds, dtype=torch.float32)).numpy()\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        elif args.output_mode == \"regression\":\n",
    "            preds = np.squeeze(preds)\n",
    "\n",
    "        if args.do_ensemble_pred:\n",
    "            result = compute_metrics(pred_task, preds, out_label_ids, probs[:,1])\n",
    "        else:\n",
    "            result = compute_metrics(pred_task, preds, out_label_ids, probs)\n",
    "        \n",
    "        pred_output_dir = args.predict_dir\n",
    "        if not os.path.exists(pred_output_dir):\n",
    "               os.makedir(pred_output_dir)\n",
    "        output_pred_file = os.path.join(pred_output_dir, \"pred_results.npy\")\n",
    "        logger.info(\"***** Pred results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        np.save(output_pred_file, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbe6d2-ef3d-4c3b-99fe-d674fb9342d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup distant debugging if needed\n",
    "    if args.server_ip and args.server_port:\n",
    "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend=\"nccl\")\n",
    "        args.n_gpu = 1\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    # Prepare GLUE task\n",
    "    args.task_name = args.task_name.lower()\n",
    "    if args.task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
    "    processor = processors[args.task_name]()\n",
    "    args.output_mode = output_modes[args.task_name]\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "    \n",
    "    args.model_type = args.model_type.lower()\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "\n",
    "    if not args.do_visualize and not args.do_ensemble_pred:\n",
    "        config = config_class.from_pretrained(\n",
    "            args.config_name if args.config_name else args.model_name_or_path,\n",
    "            num_labels=num_labels,\n",
    "            finetuning_task=args.task_name,\n",
    "            cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "        )\n",
    "        \n",
    "        config.hidden_dropout_prob = args.hidden_dropout_prob\n",
    "        config.attention_probs_dropout_prob = args.attention_probs_dropout_prob\n",
    "        if args.model_type in [\"dnalong\", \"dnalongcat\"]:\n",
    "            assert args.max_seq_length % 512 == 0\n",
    "        config.split = int(args.max_seq_length/512)\n",
    "        config.rnn = args.rnn\n",
    "        config.num_rnn_layer = args.num_rnn_layer\n",
    "        config.rnn_dropout = args.rnn_dropout\n",
    "        config.rnn_hidden = args.rnn_hidden\n",
    "\n",
    "        tokenizer = tokenizer_class.from_pretrained(\n",
    "            args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "            do_lower_case=args.do_lower_case,\n",
    "            cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "        )\n",
    "        model = model_class.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "            config=config,\n",
    "            cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "        )\n",
    "        logger.info('finish loading model')\n",
    "\n",
    "        if args.local_rank == 0:\n",
    "            torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "        model.to(args.device)\n",
    "\n",
    "        logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0) and args.task_name != \"dna690\":\n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "            os.makedirs(args.output_dir)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = model_class.from_pretrained(args.output_dir)\n",
    "        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = model_class.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    # Prediction\n",
    "    predictions = {}\n",
    "    if args.do_predict and args.local_rank in [-1, 0]:\n",
    "        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "        checkpoint = args.output_dir\n",
    "        logger.info(\"Predict using the following checkpoint: %s\", checkpoint)\n",
    "        prefix = ''\n",
    "        model = model_class.from_pretrained(checkpoint)\n",
    "        model.to(args.device)\n",
    "        prediction = predict(args, model, tokenizer, prefix=prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
