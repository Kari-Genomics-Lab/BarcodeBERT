{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUkAYZykPWel",
        "outputId": "5abc8a61-2c4b-4efa-a75d-9b65ce0be5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Datasets/BIOSCAN\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FEFWss4lOoK",
        "outputId": "19414049-77dc-458b-de25-3f8692cd6a76"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Datasets/BIOSCAN\n",
            "Bioscan.ipynb\t    medium_training.tsv    __pycache__\t\t  unseen.tsv\n",
            "Bioscan_Test.ipynb  model_checkpoints\t   small_training.tsv\n",
            "full_training.tsv   model_checkpoints_new  transformers_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4he8Zh38c4j",
        "outputId": "6f1c2156-58d9-447d-ab55-b18cfb91762b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from umap-learn) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.9/dist-packages (from umap-learn) (1.2.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/dist-packages (from umap-learn) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.9/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from umap-learn) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=c266b965b703bb53877bcbf5efa327203bcc2e33e29ffdb510d5b42534965b51\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/3e/1c/596d0a463d17475af648688443fa4846fef624d1390339e7e9\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=9e7adf061959fe45930a5683c239a47b18909e823877d4a39cc827b0567dd325\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/89/cc/59ab91ef5b21dc2ab3635528d7d227f49dfc9169905dcb959d\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.8 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the libraries\n",
        "from transformers_utils import BERT, SampleDNAData\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import umap\n",
        "import torch\n",
        "import numpy \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_pC8Pwi_iobE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers_utils import PabloDNADataset, tokenizer\n",
        "\n",
        "def get_all_kmers(k_mer_length, alphabet=None) -> list:\n",
        "\n",
        "  def base_convert(num, base, length):\n",
        "      result = []\n",
        "      while num > 0:\n",
        "          result.insert(0, num % base)\n",
        "          num = num // base\n",
        "      while len(result) < length:\n",
        "          result.insert(0, 0)\n",
        "      return result\n",
        "\n",
        "  if alphabet is None:\n",
        "      alphabet = [\"A\", \"C\", \"G\", \"T\", \"-\", \"N\"]\n",
        "  k_mer_counts = len(alphabet) ** k_mer_length\n",
        "  all_k_mers_list = []\n",
        "  for i in range(k_mer_counts):\n",
        "      code = base_convert(num=i, base=len(alphabet), length=k_mer_length)\n",
        "      k_mer = \"\"\n",
        "      for j in range(k_mer_length):\n",
        "          k_mer += alphabet[code[j]]\n",
        "      all_k_mers_list.append(k_mer)\n",
        "\n",
        "  return all_k_mers_list\n",
        "\n",
        "\n",
        "test_path = \"unseen.tsv\"\n",
        "test_dataset = PabloDNADataset(test_path)\n",
        "\n",
        "\n",
        "# Remove X,R,Y letters from data\n",
        "test_dataset.change_RXY2N(\"nucleotides\")\n",
        "test_nucleotides = list(test_dataset.df[\"nucleotides\"].values)\n",
        "test_species = list(test_dataset.df[\"species_name\"].values)\n",
        "\n",
        "word_list = get_all_kmers(4)\n",
        "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "for i, w in enumerate(word_list):\n",
        "    word_dict[w] = i + 4\n",
        "    number_dict = {i: w for i, w in enumerate(word_dict)}\n"
      ],
      "metadata": {
        "id": "eQav9Y3ewC9r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DNABERT configuration\n",
        "config = {\n",
        "    \"d_model\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"max_len\":512\n",
        "}\n",
        "\n",
        "model = BERT(len(word_dict), config[\"d_model\"], config['max_len'], 2, config[\"n_layers\"], 32, 32, config[\"n_heads\"])\n",
        "\n",
        "#Load the last model available from training\n",
        "saving_path = \"/content/drive/My Drive/Datasets/BIOSCAN/model_checkpoints_new/\"\n",
        "last_epoch = 50\n",
        "\n",
        "model.load_state_dict(torch.load(saving_path + f'model_{last_epoch}.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOkrD2wBpfaI",
        "outputId": "d5d66a2d-815b-4d6c-bb91-529e737e857d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding - GAP"
      ],
      "metadata": {
        "id": "GYKh4gsvTCHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is not the most efficient way to do this - We should have a batch and\n",
        "# a dataloader for testing. Sorry for now :(\n",
        "embedding = []\n",
        "for seq in test_nucleotides[:2500]:\n",
        "  tokens = tokenizer(seq, word_dict, 4, stride=1)\n",
        "  input_ids = [word_dict['[CLS]']] + tokens + [word_dict['[SEP]']]\n",
        "  segment_ids = [0] * (1 + len(tokens) ) + [1] * (1)\n",
        "  masked_tokens, masked_pos = [], [] #No mask for testing\n",
        "            \n",
        "  # Zero Paddings\n",
        "  n_pad = config['max_len'] - len(input_ids)\n",
        "  input_ids.extend([0] * n_pad)\n",
        "  segment_ids.extend([0] * n_pad)\n",
        "  ids, seg, msk_pos = torch.Tensor(input_ids).type(torch.IntTensor), torch.Tensor(segment_ids).type(torch.IntTensor), torch.Tensor(masked_pos).type(torch.int64)\n",
        "  #ids, seg, msk_pos = torch.cuda.IntTensor(input_ids), torch.cuda.IntTensor(segment_ids), torch.cuda.LongTensor(masked_pos)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out = model(ids.unsqueeze(1), seg.unsqueeze(1), msk_pos.unsqueeze(1))\n",
        "    z = torch.squeeze(torch.mean(out[-1], 0,False)).cpu().numpy()\n",
        "    embedding.append(z)  #GAP to compute the hidden representations\n",
        "print(len(embedding))"
      ],
      "metadata": {
        "id": "ByNzcRSJSkQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "latent = np.array(embedding)\n",
        "print(latent.shape)\n",
        "unique_labels = list(np.unique(test_species[:2500]))\n",
        "y_true = np.array(list(map(lambda x: unique_labels.index(x), test_species[:50])))\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1) \n",
        "ax.set_title(\"Representation of the Latent Space\")\n",
        "ax.set_xlabel(\"UMAP 1\")\n",
        "ax.set_ylabel(\"UMAP 2\")\n",
        "\n",
        "_embedding = umap.UMAP(random_state=42).fit_transform(latent)\n",
        "print(_embedding.shape)\n",
        "\n",
        "ax.scatter(_embedding[:, 0],\n",
        "           _embedding[:, 1],\n",
        "           c=y_true,\n",
        "           s=1,\n",
        "           cmap='Spectral')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lsT53-dF2UoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Histogram of Nearest Neighbors\n",
        "metrics = ['manhattan', 'cosine', 'minkowski']\n",
        "scores = []\n",
        "neighbour_size = 1\n",
        "gt = y_true\n",
        "for metric in metrics:\n",
        "    nbrs = NearestNeighbors(n_neighbors=neighbour_size+1, metric=metric).fit(embedding)\n",
        "    _, neighbour_indices = nbrs.kneighbors(embedding)\n",
        "    neighbour_indices = neighbour_indices.astype(np.int32)[:,1:]\n",
        "    #print(neighbour_indices)\n",
        "    neighbour_indices = neighbour_indices.reshape(-1)\n",
        "    #print(neighbour_indices)\n",
        "      \n",
        "    gt_indices = np.array([[idx]*neighbour_size for idx in range(len(gt))]).reshape(-1).astype(np.int32)\n",
        "    #print(gt_indices.shape)\n",
        "\n",
        "    gt = np.array(gt)\n",
        "\n",
        "    neighbor_gt = gt[neighbour_indices]\n",
        "    #print(np.sum(gt[gt_indices] == gt[neighbour_indices]))\n",
        "\n",
        "    cluster_distribution = pd.Series(gt[gt_indices]).value_counts().to_dict()\n",
        "    #print(cluster_distribution)\n",
        "    good_neighbours = pd.Series( gt[gt_indices][gt[gt_indices] == gt[neighbour_indices]]).value_counts().to_dict()\n",
        "    #print(good_neighbours)\n",
        "    score = 0\n",
        "    n_clusters = 0\n",
        "    for cluster in cluster_distribution:\n",
        "        score += good_neighbours[cluster]/cluster_distribution[cluster]\n",
        "        n_clusters += 1\n",
        "    scores.append(score/n_clusters)\n",
        "\n",
        "#print(scores)\n",
        "print(\"Best Metric: \", metrics[scores.index(max(scores))], \"Accuracy: \", max(scores))\n",
        "best_metric = metrics[scores.index(max(scores))]"
      ],
      "metadata": {
        "id": "EhaRszumHiVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding - ['CLS']"
      ],
      "metadata": {
        "id": "hK01CuDy1aFD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bR0_RhA61Xe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "THOW1ieXCois"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding - MAP\n"
      ],
      "metadata": {
        "id": "stu7yxTh1eUo"
      }
    }
  ]
}