{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# requirements"
      ],
      "metadata": {
        "id": "EN3EsTEzIUcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import random\n",
        "from random import randrange, shuffle, random, randint\n",
        "import re\n",
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "AueLRah0IT-M"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data and tokenize"
      ],
      "metadata": {
        "id": "1lURUvE3HuCX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7F8QcTd5HV6a"
      },
      "outputs": [],
      "source": [
        "class PabloDNADataset:\n",
        "    def __init__(self, file_path):\n",
        "        self.df = pd.read_csv(file_path, sep=\"\\t\", encoding=\"unicode_escape\")\n",
        "\n",
        "    def clean_nan(self, col_names, replace_orig=False):\n",
        "        clean_df = self.df.dropna(subset=col_names)\n",
        "        clean_df = clean_df.reset_index(drop=True)\n",
        "        if replace_orig:\n",
        "            self.df = clean_df\n",
        "        return clean_df\n",
        "    \n",
        "    def change_RXY2N(self, col_names, replace_orig=False):\n",
        "      full_pattern = re.compile('[^ACGTN\\-]') \n",
        "      self.df[col_names] = self.df[col_names].apply(lambda x: re.sub(full_pattern, 'N', x))\n",
        "      # if replace_orig:\n",
        "      #   self.df[col_names] = clean_nucleotides\n",
        "      # return clean_str_df\n",
        "\n",
        "\n",
        "    def generate_mini_sample(self, dataframe=None, bin_count=20, output_path=\"mini_sample.tsv\"):\n",
        "        if dataframe is None:\n",
        "            dataframe = self.df\n",
        "        bins = list(dataframe['bin_uri'].unique())\n",
        "        rd1 = random.sample(range(0, len(bins)), bin_count)\n",
        "        bins = [bins[i] for i in rd1]\n",
        "        mini_df = dataframe.loc[dataframe['bin_uri'].isin(bins)]\n",
        "        mini_df = mini_df.reset_index(drop=True)\n",
        "        # mini_df = dataframe.iloc[0:sample_count]\n",
        "        # mini_df = dataframe.take(np.random.permutation(len(dataframe))[:sample_count])\n",
        "        mini_df.to_csv(output_path, sep=\"\\t\")\n",
        "\n",
        "    def get_info(self, dataframe=None):\n",
        "        if dataframe is None:\n",
        "            dataframe = self.df\n",
        "        print(\"Total data: \", len(dataframe))\n",
        "        print(\"Number of bin clusters: \", len(dataframe['bin_uri'].unique()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(dna_sentence, k_mer_dict, k_mer_length, stride=1):\n",
        "    tokens = []\n",
        "    for i in range(0, len(dna_sentence) - k_mer_length + 1, stride):\n",
        "        k_mer = dna_sentence[i:i + k_mer_length]\n",
        "        tokens.append(k_mer_dict[k_mer])\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class SampleDNAData(Dataset):\n",
        "    \"\"\"Barcode Dataset\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_all_kmers(k_mer_length, alphabet=None) -> list:\n",
        "        \"\"\"\n",
        "        :rtype: object\n",
        "        \"\"\"\n",
        "        def base_convert(num, base, length):\n",
        "            result = []\n",
        "            while num > 0:\n",
        "                result.insert(0, num % base)\n",
        "                num = num // base\n",
        "            while len(result) < length:\n",
        "                result.insert(0, 0)\n",
        "            return result\n",
        "\n",
        "        if alphabet is None:\n",
        "            alphabet = [\"A\", \"C\", \"G\", \"T\", \"-\", \"N\"]\n",
        "        k_mer_counts = len(alphabet) ** k_mer_length\n",
        "        all_k_mers_list = []\n",
        "        for i in range(k_mer_counts):\n",
        "            code = base_convert(num=i, base=len(alphabet), length=k_mer_length)\n",
        "            k_mer = \"\"\n",
        "            for j in range(k_mer_length):\n",
        "                k_mer += alphabet[code[j]]\n",
        "            all_k_mers_list.append(k_mer)\n",
        "\n",
        "        return all_k_mers_list\n",
        "\n",
        "    \n",
        "    def __init__(self, file_path, k_mer=4, data_count=None, max_mask_count=5, max_len=256):\n",
        "        self.k_mer = k_mer\n",
        "        pablo_dataset = PabloDNADataset(file_path)\n",
        "        if data_count is None:\n",
        "            data_count = len(pablo_dataset.df)\n",
        "        # for removing X,R,Y letters from data\n",
        "        # pablo_dataset.change_RXY2N(\"nucleotides\")\n",
        "        self.dna_nucleotides = list(pablo_dataset.df[\"nucleotides\"].values)\n",
        "        word_list = SampleDNAData.get_all_kmers(self.k_mer)\n",
        "\n",
        "        number_dict = dict()\n",
        "        word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "        for i, w in enumerate(word_list):\n",
        "            word_dict[w] = i + 4\n",
        "            number_dict = {i: w for i, w in enumerate(word_dict)}\n",
        "\n",
        "        self.word_dict = word_dict\n",
        "        self.number_dict = number_dict\n",
        "\n",
        "        self.vocab_size = len(word_dict)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.batch = []\n",
        "        positive = negative = 0\n",
        "        while positive != data_count / 2 or negative != data_count / 2:\n",
        "            is_positive = randrange(0, 2)\n",
        "\n",
        "            tokens_a_index, tokens_b_index = 0, 0\n",
        "            while tokens_a_index == tokens_b_index:\n",
        "                tokens_a_index, tokens_b_index = randrange(len(self.dna_nucleotides)), randrange(\n",
        "                    len(self.dna_nucleotides))\n",
        "\n",
        "            if is_positive:\n",
        "                dna_a = self.dna_nucleotides[tokens_a_index]\n",
        "                dna_b = dna_a\n",
        "            else:\n",
        "                dna_a = self.dna_nucleotides[tokens_a_index]\n",
        "                dna_b = self.dna_nucleotides[tokens_b_index]\n",
        "\n",
        "            rand_len = randrange(128, 256)\n",
        "\n",
        "            dna_a = dna_a[0:len(dna_a) // 2][0:rand_len]  # max_len//2 - 3]\n",
        "            dna_b = dna_b[len(dna_b) // 2:][0:rand_len]  # max_len//2 - 3]\n",
        "            tokens_a = tokenizer(dna_a, word_dict, k_mer, stride=1)\n",
        "            tokens_b = tokenizer(dna_b, word_dict, k_mer, stride=1)\n",
        "            input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
        "            segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "            # MASK LM\n",
        "            n_pred = min(max_mask_count, max(1, int(round(len(input_ids) * 0.15)))) // self.k_mer  # 15 % of tokens in one sentence\n",
        "            cand_masked_pos = [i for i, token in enumerate(input_ids)\n",
        "                               if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
        "\n",
        "            # remove N and gaps from cand_masked_pos\n",
        "            cand_masked_pos_copy = cand_masked_pos.copy()\n",
        "            for position in cand_masked_pos_copy:\n",
        "                remove_flag = False\n",
        "                for s in range(self.k_mer):\n",
        "                    if position + s < len(input_ids):\n",
        "                        key = self.number_dict[input_ids[position + s]]\n",
        "                        if (\"N\" in key) or (\"-\" in key):\n",
        "                            remove_flag = True\n",
        "                            break\n",
        "                if remove_flag:\n",
        "                    cand_masked_pos.remove(position)\n",
        "\n",
        "            shuffle(cand_masked_pos)\n",
        "\n",
        "            # if the position remains is less than 15%, mask them all\n",
        "            if len(cand_masked_pos) < n_pred:\n",
        "                n_pred = len(cand_masked_pos)\n",
        "\n",
        "            masked_tokens, masked_pos = [], []\n",
        "            for pos in cand_masked_pos[:n_pred]:\n",
        "                for s in range(self.k_mer):\n",
        "                    if pos + s < len(input_ids):\n",
        "                        masked_pos.append(pos + s)\n",
        "                        masked_tokens.append(input_ids[pos + s])\n",
        "                        input_ids[pos + s] = word_dict['[MASK]']  # make mask\n",
        "\n",
        "            # Zero Paddings\n",
        "            n_pad = max_len - len(input_ids)\n",
        "            input_ids.extend([0] * n_pad)\n",
        "            segment_ids.extend([0] * n_pad)\n",
        "\n",
        "            # Zero Padding (100% - 15%) tokens\n",
        "            if max_mask_count > len(masked_pos):\n",
        "                n_pad = max_mask_count - len(masked_pos)\n",
        "                masked_tokens.extend([0] * n_pad)\n",
        "                masked_pos.extend([0] * n_pad)\n",
        "\n",
        "            if is_positive and positive < data_count / 2:\n",
        "                self.batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])  # IsNext\n",
        "                positive += 1\n",
        "            elif not is_positive and negative < data_count / 2:\n",
        "                self.batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])  # NotNext\n",
        "                negative += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ids = torch.Tensor(self.batch[idx][0])\n",
        "        seg = torch.Tensor(self.batch[idx][1])\n",
        "        msk_tok = torch.Tensor(self.batch[idx][2])\n",
        "        msk_pos = torch.Tensor(self.batch[idx][3])\n",
        "        label = torch.Tensor([self.batch[idx][4]])\n",
        "\n",
        "        ids, seg, msk_pos = ids.type(torch.IntTensor), seg.type(torch.IntTensor), msk_pos.type(torch.int64)\n",
        "\n",
        "        msk_tok = msk_tok.type(torch.LongTensor)\n",
        "        label = label.type(torch.LongTensor)\n",
        "\n",
        "        return ids, seg, msk_pos, msk_tok, label\n"
      ],
      "metadata": {
        "id": "WuvfJQ4MINGk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "EEHIASapIyGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, maxlen, n_segments, device):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(int(vocab_size), int(d_model))  # token embedding\n",
        "        self.pos_embed = nn.Embedding(int(maxlen), int(d_model))  # position embedding\n",
        "        self.seg_embed = nn.Embedding(int(n_segments), int(d_model))  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(int(d_model))\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long, device=self.device)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "def get_attn_pad_mask(seq_q, seq_k, device):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    pad_attn_mask = pad_attn_mask.to(device)\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, n_heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads)\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_k)\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)  # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs)  # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, d_k, d_v, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.linear = nn.Linear(self.n_heads * self.d_v, self.d_model)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)  # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)  # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = self.linear(context)\n",
        "\n",
        "        return self.layernorm(output + residual), attn  # output: [batch_size x len_q x d_model]\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.l1 = nn.Linear(d_model, d_ff)\n",
        "        self.l2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "        self.relu = GELU()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        residual = inputs\n",
        "        output = self.l1(inputs)\n",
        "        output = self.relu(output)\n",
        "        output = self.l2(output)\n",
        "        return self.layer_norm(output + residual)\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k):\n",
        "        self.d_k = d_k\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(\n",
        "            self.d_k)  # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9)  # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, maxlen, n_segments, n_layers, d_k, d_v, n_heads, device):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, d_model, maxlen, n_segments, device)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, d_k, d_v, n_heads) for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ1 = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = GELU()\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "        self.device=device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
        "        for layer in self.layers:\n",
        "            # embedding layer\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        # classifier pos/neg (it will be decided by first token(CLS))\n",
        "        h_pooled = self.activ1(self.fc(output[:, 0]))  # [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled)  # [batch_size, 2]\n",
        "\n",
        "        # classifier mask\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))  # [batch_size, max_pred, d_model]\n",
        "        # get masked position from final output of transformer.\n",
        "        h_masked = torch.gather(output, 1, masked_pos)  # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias  # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_clsf, output"
      ],
      "metadata": {
        "id": "jnNFTO3xIzcO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n"
      ],
      "metadata": {
        "id": "WCsAwf11JJDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your drive to access the data"
      ],
      "metadata": {
        "id": "OkKDUYSeWapx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKfdWluKJrMs",
        "outputId": "a0e8ebc7-48ae-401b-92e6-1131e5d3742a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "NQBtv4hBP7cc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File path of the input\n",
        "# input_path = \"/content/drive/MyDrive/BIOSCAN/full_training.tsv\"\n",
        "# input_path = \"/content/drive/MyDrive/BIOSCAN/medium_training.tsv\"\n",
        "input_path = \"/content/drive/MyDrive/BIOSCAN/small_training.tsv\"\n",
        "\n",
        "\n",
        "# dataloader to get a batch of data\n",
        "dataset = SampleDNAData(file_path=input_path, k_mer=4, data_count=256, max_mask_count=80, max_len=512)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# define the model\n",
        "# DNABert configuration\n",
        "config = {\n",
        "    \"d_model\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12 \n",
        "\n",
        "}\n",
        "# # first config\n",
        "# config = {\n",
        "#     \"d_model\": 16,\n",
        "#     \"n_heads\": 2,\n",
        "#     \"n_layers\": 5 \n",
        "\n",
        "# }\n",
        "model = BERT(dataset.vocab_size, config[\"d_model\"], dataset.max_len, 2, config[\"n_layers\"], 32, 32, config[\"n_heads\"], device=device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "hEshydwlBusq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saving_path = \"/content/drive/MyDrive/BIOSCAN/model_checkpoints/\"\n",
        "epoch_loss_list = []\n",
        "training_epoch = 1000\n",
        "continue_epoch = 0\n",
        "\n",
        "''' \n",
        "If you lost your connection and want to continue the training process, \n",
        "uncomment this part, load your last model, optimizer, and loss, \n",
        "choose the epoch you \n",
        "want to continue from\n",
        "'''\n",
        "# continue_epoch = 100\n",
        "# model.load_state_dict(torch.load(saving_path + f'model_{continue_epoch}.pth'))\n",
        "# optimizer.load_state_dict(torch.load(saving_path + f\"optimizer_{continue_epoch}.pth\"))\n",
        "\n",
        "# a_file = open(saving_path + \"loss.pkl\", \"rb\")\n",
        "# epoch_loss_list = pickle.load(a_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "UR3ajJsf4ZN3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3fe3b95c-f51a-4a4c-d846-566a913833c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nIf you lost your connection and want to continue the training process, \\nuncomment this part, load your last model, optimizer, and loss, \\nchoose the epoch you \\nwant to continue from\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# strat training\n",
        "for epoch in range(continue_epoch, training_epoch + 1):\n",
        "    epoch_loss = 0\n",
        "    for ids, seg, msk_pos, masked_tokens, is_pos in dataloader:\n",
        "        ids = ids.to(device)\n",
        "        seg = seg.to(device)\n",
        "        msk_pos = msk_pos.to(device)\n",
        "        masked_tokens = masked_tokens.to(device)\n",
        "        is_pos = is_pos.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits_lm, logits_clsf, outputs= model(ids, seg, msk_pos)\n",
        "\n",
        "        loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)  # for masked LM\n",
        "        loss_lm = (loss_lm.float()).mean()\n",
        "        loss_clsf = criterion(logits_clsf, torch.squeeze(is_pos))  # for sentence classification\n",
        "        loss = loss_lm + loss_clsf\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    epoch_loss_list.append(epoch_loss)\n",
        "\n",
        "    print(f\"epoch {epoch}: Loss is {epoch_loss}\")\n",
        "\n",
        "    # every 50 epoch save the checkpoints and save the loss in a list\n",
        "    if epoch % 50 == 0:\n",
        "        torch.save(model.state_dict(), saving_path + \"model_\" + str(epoch) +'.pth')\n",
        "        torch.save(optimizer.state_dict(), saving_path + \"optimizer_\" + str(epoch) +'.pth')\n",
        "\n",
        "        a_file = open(saving_path + \"loss.pkl\", \"wb\")\n",
        "        pickle.dump(epoch_loss_list, a_file)\n",
        "        a_file.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLAbcratJLHT",
        "outputId": "353df421-20ae-461d-f89c-5dcc4efa7fe2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0: Loss is 2358.167432785034\n",
            "epoch 1: Loss is 528.3459033966064\n",
            "epoch 2: Loss is 393.48581981658936\n",
            "epoch 3: Loss is 360.1351442337036\n",
            "epoch 4: Loss is 358.3264570236206\n",
            "epoch 5: Loss is 369.4758577346802\n",
            "epoch 6: Loss is 341.71941471099854\n",
            "epoch 7: Loss is 345.9011182785034\n",
            "epoch 8: Loss is 341.4824810028076\n",
            "epoch 9: Loss is 338.59256076812744\n",
            "epoch 10: Loss is 336.25152254104614\n",
            "epoch 11: Loss is 334.93188428878784\n",
            "epoch 12: Loss is 333.64481019973755\n",
            "epoch 13: Loss is 332.3218665122986\n",
            "epoch 14: Loss is 331.03067445755005\n",
            "epoch 15: Loss is 329.87801694869995\n",
            "epoch 16: Loss is 329.0447030067444\n",
            "epoch 17: Loss is 328.59162616729736\n",
            "epoch 18: Loss is 328.3054504394531\n",
            "epoch 19: Loss is 328.2332649230957\n",
            "epoch 20: Loss is 328.1945013999939\n",
            "epoch 21: Loss is 328.5982394218445\n",
            "epoch 22: Loss is 328.5405373573303\n",
            "epoch 23: Loss is 328.31352186203003\n",
            "epoch 24: Loss is 327.8445954322815\n",
            "epoch 25: Loss is 327.6662220954895\n",
            "epoch 26: Loss is 327.2606167793274\n",
            "epoch 27: Loss is 327.06008768081665\n",
            "epoch 28: Loss is 326.8321557044983\n",
            "epoch 29: Loss is 326.610196352005\n",
            "epoch 30: Loss is 326.38076734542847\n",
            "epoch 31: Loss is 326.0969545841217\n",
            "epoch 32: Loss is 325.93354845046997\n",
            "epoch 33: Loss is 325.68018198013306\n",
            "epoch 34: Loss is 325.53256916999817\n",
            "epoch 35: Loss is 325.3456106185913\n",
            "epoch 36: Loss is 325.22749757766724\n",
            "epoch 37: Loss is 325.02679347991943\n",
            "epoch 38: Loss is 324.8726851940155\n",
            "epoch 39: Loss is 324.73127818107605\n",
            "epoch 40: Loss is 324.60513973236084\n",
            "epoch 41: Loss is 324.4179265499115\n",
            "epoch 42: Loss is 324.2913386821747\n",
            "epoch 43: Loss is 324.1333475112915\n",
            "epoch 44: Loss is 324.04028367996216\n",
            "epoch 45: Loss is 323.9523763656616\n",
            "epoch 46: Loss is 323.8364396095276\n",
            "epoch 47: Loss is 323.6581828594208\n",
            "epoch 48: Loss is 323.5497260093689\n",
            "epoch 49: Loss is 323.3901879787445\n",
            "epoch 50: Loss is 323.34373140335083\n",
            "epoch 51: Loss is 323.2197344303131\n",
            "epoch 52: Loss is 323.0933082103729\n",
            "epoch 53: Loss is 323.00263714790344\n",
            "epoch 54: Loss is 322.89885091781616\n",
            "epoch 55: Loss is 322.84388995170593\n",
            "epoch 56: Loss is 322.796546459198\n",
            "epoch 57: Loss is 322.6144320964813\n",
            "epoch 58: Loss is 322.5158553123474\n",
            "epoch 59: Loss is 322.44931745529175\n",
            "epoch 60: Loss is 322.42524242401123\n",
            "epoch 61: Loss is 322.335755109787\n",
            "epoch 62: Loss is 322.2670359611511\n",
            "epoch 63: Loss is 322.1697542667389\n",
            "epoch 64: Loss is 322.0941996574402\n",
            "epoch 65: Loss is 322.0019145011902\n",
            "epoch 66: Loss is 321.8788757324219\n",
            "epoch 67: Loss is 321.86117458343506\n",
            "epoch 68: Loss is 321.7516145706177\n",
            "epoch 69: Loss is 321.6970911026001\n",
            "epoch 70: Loss is 321.6287040710449\n",
            "epoch 71: Loss is 321.55383253097534\n",
            "epoch 72: Loss is 321.5337989330292\n",
            "epoch 73: Loss is 321.4634256362915\n",
            "epoch 74: Loss is 321.3970127105713\n",
            "epoch 75: Loss is 321.30513739585876\n",
            "epoch 76: Loss is 321.22078585624695\n",
            "epoch 77: Loss is 321.17450761795044\n",
            "epoch 78: Loss is 321.1161732673645\n",
            "epoch 79: Loss is 321.02064418792725\n",
            "epoch 80: Loss is 321.0111873149872\n",
            "epoch 81: Loss is 321.0003526210785\n",
            "epoch 82: Loss is 320.95427107810974\n",
            "epoch 83: Loss is 320.8600380420685\n",
            "epoch 84: Loss is 320.8153235912323\n",
            "epoch 85: Loss is 320.72553968429565\n",
            "epoch 86: Loss is 320.60767102241516\n",
            "epoch 87: Loss is 320.6115894317627\n",
            "epoch 88: Loss is 320.658166885376\n",
            "epoch 89: Loss is 320.57790660858154\n",
            "epoch 90: Loss is 320.43856167793274\n",
            "epoch 91: Loss is 320.4308352470398\n",
            "epoch 92: Loss is 320.4544687271118\n",
            "epoch 93: Loss is 320.3591728210449\n",
            "epoch 94: Loss is 320.27459740638733\n",
            "epoch 95: Loss is 320.19202756881714\n",
            "epoch 96: Loss is 320.18124985694885\n",
            "epoch 97: Loss is 320.14602398872375\n",
            "epoch 98: Loss is 320.07058238983154\n",
            "epoch 99: Loss is 320.1474781036377\n",
            "epoch 100: Loss is 320.0634458065033\n",
            "epoch 101: Loss is 319.904904127121\n",
            "epoch 102: Loss is 319.7795042991638\n",
            "epoch 103: Loss is 319.79427433013916\n",
            "epoch 104: Loss is 319.69495487213135\n",
            "epoch 105: Loss is 319.6691610813141\n",
            "epoch 106: Loss is 319.5749945640564\n",
            "epoch 107: Loss is 319.56073474884033\n",
            "epoch 108: Loss is 319.5083918571472\n",
            "epoch 109: Loss is 319.47842383384705\n",
            "epoch 110: Loss is 319.42871356010437\n",
            "epoch 111: Loss is 319.3647131919861\n",
            "epoch 112: Loss is 319.32741260528564\n",
            "epoch 113: Loss is 319.2853002548218\n",
            "epoch 114: Loss is 319.25998878479004\n",
            "epoch 115: Loss is 319.23552417755127\n",
            "epoch 116: Loss is 319.14995527267456\n",
            "epoch 117: Loss is 319.1086859703064\n",
            "epoch 118: Loss is 319.08896017074585\n",
            "epoch 119: Loss is 319.02761340141296\n",
            "epoch 120: Loss is 318.97372221946716\n",
            "epoch 121: Loss is 318.9820463657379\n",
            "epoch 122: Loss is 318.91847372055054\n",
            "epoch 123: Loss is 318.8916697502136\n",
            "epoch 124: Loss is 318.833696603775\n",
            "epoch 125: Loss is 318.77837800979614\n",
            "epoch 126: Loss is 318.7714443206787\n",
            "epoch 127: Loss is 318.7316508293152\n",
            "epoch 128: Loss is 318.6849226951599\n",
            "epoch 129: Loss is 318.6599597930908\n",
            "epoch 130: Loss is 318.6183428764343\n",
            "epoch 131: Loss is 318.5736508369446\n",
            "epoch 132: Loss is 318.5107388496399\n",
            "epoch 133: Loss is 318.5073871612549\n",
            "epoch 134: Loss is 318.47870230674744\n",
            "epoch 135: Loss is 318.4302821159363\n",
            "epoch 136: Loss is 318.38443756103516\n",
            "epoch 137: Loss is 318.3511550426483\n",
            "epoch 138: Loss is 318.310426235199\n",
            "epoch 139: Loss is 318.26236295700073\n",
            "epoch 140: Loss is 318.2435541152954\n",
            "epoch 141: Loss is 318.16426253318787\n",
            "epoch 142: Loss is 318.14247488975525\n",
            "epoch 143: Loss is 318.1166682243347\n",
            "epoch 144: Loss is 318.0987148284912\n",
            "epoch 145: Loss is 318.0587992668152\n",
            "epoch 146: Loss is 318.0131893157959\n",
            "epoch 147: Loss is 317.9824056625366\n",
            "epoch 148: Loss is 317.9410741329193\n",
            "epoch 149: Loss is 317.8929307460785\n",
            "epoch 150: Loss is 317.8671998977661\n",
            "epoch 151: Loss is 317.7848632335663\n",
            "epoch 152: Loss is 317.78947591781616\n",
            "epoch 153: Loss is 317.7592227458954\n",
            "epoch 154: Loss is 317.71688580513\n",
            "epoch 155: Loss is 317.67671942710876\n",
            "epoch 156: Loss is 317.66842246055603\n",
            "epoch 157: Loss is 317.64845967292786\n",
            "epoch 158: Loss is 317.6323666572571\n",
            "epoch 159: Loss is 317.61609864234924\n",
            "epoch 160: Loss is 317.5801258087158\n",
            "epoch 161: Loss is 317.5869348049164\n",
            "epoch 162: Loss is 317.5763385295868\n",
            "epoch 163: Loss is 317.569034576416\n",
            "epoch 164: Loss is 317.55811047554016\n",
            "epoch 165: Loss is 317.53384828567505\n",
            "epoch 166: Loss is 317.5484914779663\n",
            "epoch 167: Loss is 317.5408658981323\n",
            "epoch 168: Loss is 317.53038454055786\n",
            "epoch 169: Loss is 317.5152099132538\n",
            "epoch 170: Loss is 317.48286604881287\n",
            "epoch 171: Loss is 317.4644684791565\n",
            "epoch 172: Loss is 317.43283796310425\n",
            "epoch 173: Loss is 317.4005241394043\n",
            "epoch 174: Loss is 317.3642416000366\n",
            "epoch 175: Loss is 317.3227496147156\n",
            "epoch 176: Loss is 317.2722396850586\n",
            "epoch 177: Loss is 317.24264121055603\n",
            "epoch 178: Loss is 317.20140504837036\n",
            "epoch 179: Loss is 317.16967582702637\n",
            "epoch 180: Loss is 317.1285035610199\n",
            "epoch 181: Loss is 317.1084043979645\n",
            "epoch 182: Loss is 317.08258628845215\n",
            "epoch 183: Loss is 317.0434329509735\n",
            "epoch 184: Loss is 317.0219180583954\n",
            "epoch 185: Loss is 317.01342511177063\n",
            "epoch 186: Loss is 317.0016553401947\n",
            "epoch 187: Loss is 316.98986864089966\n",
            "epoch 188: Loss is 316.9724943637848\n",
            "epoch 189: Loss is 316.9618718624115\n",
            "epoch 190: Loss is 316.96782875061035\n",
            "epoch 191: Loss is 316.98514223098755\n",
            "epoch 192: Loss is 317.00277304649353\n",
            "epoch 193: Loss is 317.0217957496643\n",
            "epoch 194: Loss is 317.0312383174896\n",
            "epoch 195: Loss is 317.06176686286926\n",
            "epoch 196: Loss is 317.0818955898285\n",
            "epoch 197: Loss is 317.0918769836426\n",
            "epoch 198: Loss is 317.1059944629669\n",
            "epoch 199: Loss is 317.11280155181885\n",
            "epoch 200: Loss is 317.0634078979492\n",
            "epoch 201: Loss is 317.0857467651367\n",
            "epoch 202: Loss is 317.0642409324646\n",
            "epoch 203: Loss is 317.026816368103\n",
            "epoch 204: Loss is 316.9790599346161\n",
            "epoch 205: Loss is 316.92260241508484\n",
            "epoch 206: Loss is 316.8614830970764\n",
            "epoch 207: Loss is 316.7986726760864\n",
            "epoch 208: Loss is 316.72690439224243\n",
            "epoch 209: Loss is 316.67272686958313\n",
            "epoch 210: Loss is 316.6176929473877\n",
            "epoch 211: Loss is 316.56403827667236\n",
            "epoch 212: Loss is 316.49287033081055\n",
            "epoch 213: Loss is 316.46317768096924\n",
            "epoch 214: Loss is 316.41760444641113\n",
            "epoch 215: Loss is 316.3720099925995\n",
            "epoch 216: Loss is 316.32784843444824\n",
            "epoch 217: Loss is 316.27643942832947\n",
            "epoch 218: Loss is 316.24195623397827\n",
            "epoch 219: Loss is 316.2074658870697\n",
            "epoch 220: Loss is 316.17224407196045\n",
            "epoch 221: Loss is 316.1387937068939\n",
            "epoch 222: Loss is 316.1069827079773\n",
            "epoch 223: Loss is 316.06178092956543\n",
            "epoch 224: Loss is 316.04486203193665\n",
            "epoch 225: Loss is 316.01924896240234\n",
            "epoch 226: Loss is 315.9923143386841\n",
            "epoch 227: Loss is 315.95619678497314\n",
            "epoch 228: Loss is 315.92289304733276\n",
            "epoch 229: Loss is 315.9047245979309\n",
            "epoch 230: Loss is 315.88625717163086\n",
            "epoch 231: Loss is 315.8659710884094\n",
            "epoch 232: Loss is 315.8405611515045\n",
            "epoch 233: Loss is 315.8181698322296\n",
            "epoch 234: Loss is 315.7949833869934\n",
            "epoch 235: Loss is 315.77354764938354\n",
            "epoch 236: Loss is 315.752809047699\n",
            "epoch 237: Loss is 315.73213624954224\n",
            "epoch 238: Loss is 315.70268654823303\n",
            "epoch 239: Loss is 315.68869161605835\n",
            "epoch 240: Loss is 315.6709644794464\n",
            "epoch 241: Loss is 315.6512532234192\n",
            "epoch 242: Loss is 315.6313877105713\n",
            "epoch 243: Loss is 315.6122934818268\n",
            "epoch 244: Loss is 315.59390354156494\n",
            "epoch 245: Loss is 315.56765580177307\n",
            "epoch 246: Loss is 315.55393171310425\n",
            "epoch 247: Loss is 315.53530073165894\n",
            "epoch 248: Loss is 315.5161929130554\n",
            "epoch 249: Loss is 315.4997458457947\n",
            "epoch 250: Loss is 315.48543643951416\n",
            "epoch 251: Loss is 315.4661936759949\n",
            "epoch 252: Loss is 315.4365963935852\n",
            "epoch 253: Loss is 315.4281885623932\n",
            "epoch 254: Loss is 315.41421604156494\n",
            "epoch 255: Loss is 315.39642691612244\n",
            "epoch 256: Loss is 315.37813425064087\n",
            "epoch 257: Loss is 315.3601768016815\n",
            "epoch 258: Loss is 315.34222984313965\n",
            "epoch 259: Loss is 315.3244514465332\n",
            "epoch 260: Loss is 315.30543756484985\n",
            "epoch 261: Loss is 315.2904677391052\n",
            "epoch 262: Loss is 315.27334690093994\n",
            "epoch 263: Loss is 315.25616002082825\n",
            "epoch 264: Loss is 315.2387328147888\n",
            "epoch 265: Loss is 315.2037401199341\n",
            "epoch 266: Loss is 315.19868326187134\n",
            "epoch 267: Loss is 315.1828031539917\n",
            "epoch 268: Loss is 315.1590824127197\n",
            "epoch 269: Loss is 315.1464469432831\n",
            "epoch 270: Loss is 315.1293077468872\n",
            "epoch 271: Loss is 315.1010847091675\n",
            "epoch 272: Loss is 315.0982737541199\n",
            "epoch 273: Loss is 315.075501203537\n",
            "epoch 274: Loss is 315.0657377243042\n",
            "epoch 275: Loss is 315.05454301834106\n",
            "epoch 276: Loss is 315.03982949256897\n",
            "epoch 277: Loss is 315.02491760253906\n",
            "epoch 278: Loss is 315.0098090171814\n",
            "epoch 279: Loss is 314.9835522174835\n",
            "epoch 280: Loss is 314.97829008102417\n",
            "epoch 281: Loss is 314.9646773338318\n",
            "epoch 282: Loss is 314.94799876213074\n",
            "epoch 283: Loss is 314.9341413974762\n",
            "epoch 284: Loss is 314.9194724559784\n",
            "epoch 285: Loss is 314.9051778316498\n",
            "epoch 286: Loss is 314.89132046699524\n",
            "epoch 287: Loss is 314.87764501571655\n",
            "epoch 288: Loss is 314.8649251461029\n",
            "epoch 289: Loss is 314.8517589569092\n",
            "epoch 290: Loss is 314.8404583930969\n",
            "epoch 291: Loss is 314.8298625946045\n",
            "epoch 292: Loss is 314.8181188106537\n",
            "epoch 293: Loss is 314.8075592517853\n",
            "epoch 294: Loss is 314.798357963562\n",
            "epoch 295: Loss is 314.789021730423\n",
            "epoch 296: Loss is 314.78085374832153\n",
            "epoch 297: Loss is 314.7717125415802\n",
            "epoch 298: Loss is 314.7636115550995\n",
            "epoch 299: Loss is 314.75609612464905\n",
            "epoch 300: Loss is 314.7492485046387\n",
            "epoch 301: Loss is 314.74317502975464\n",
            "epoch 302: Loss is 314.73779487609863\n",
            "epoch 303: Loss is 314.73340678215027\n",
            "epoch 304: Loss is 314.7303969860077\n",
            "epoch 305: Loss is 314.72660994529724\n",
            "epoch 306: Loss is 314.7239098548889\n",
            "epoch 307: Loss is 314.72130608558655\n",
            "epoch 308: Loss is 314.7186405658722\n",
            "epoch 309: Loss is 314.71564269065857\n",
            "epoch 310: Loss is 314.7126703262329\n",
            "epoch 311: Loss is 314.709673166275\n",
            "epoch 312: Loss is 314.70653223991394\n",
            "epoch 313: Loss is 314.7042770385742\n",
            "epoch 314: Loss is 314.70224475860596\n",
            "epoch 315: Loss is 314.69975900650024\n",
            "epoch 316: Loss is 314.69627022743225\n",
            "epoch 317: Loss is 314.69224739074707\n",
            "epoch 318: Loss is 314.6856620311737\n",
            "epoch 319: Loss is 314.6783752441406\n",
            "epoch 320: Loss is 314.670401096344\n",
            "epoch 321: Loss is 314.6615700721741\n",
            "epoch 322: Loss is 314.65158581733704\n",
            "epoch 323: Loss is 314.63946080207825\n",
            "epoch 324: Loss is 314.6263542175293\n",
            "epoch 325: Loss is 314.61181116104126\n",
            "epoch 326: Loss is 314.5928032398224\n",
            "epoch 327: Loss is 314.5814051628113\n",
            "epoch 328: Loss is 314.56696677207947\n",
            "epoch 329: Loss is 314.5511271953583\n",
            "epoch 330: Loss is 314.5344157218933\n",
            "epoch 331: Loss is 314.516930103302\n",
            "epoch 332: Loss is 314.499365568161\n",
            "epoch 333: Loss is 314.48174142837524\n",
            "epoch 334: Loss is 314.46434354782104\n",
            "epoch 335: Loss is 314.4467680454254\n",
            "epoch 336: Loss is 314.42979311943054\n",
            "epoch 337: Loss is 314.4115409851074\n",
            "epoch 338: Loss is 314.3948414325714\n",
            "epoch 339: Loss is 314.37127780914307\n",
            "epoch 340: Loss is 314.3566663265228\n",
            "epoch 341: Loss is 314.34332823753357\n",
            "epoch 342: Loss is 314.325647354126\n",
            "epoch 343: Loss is 314.3100299835205\n",
            "epoch 344: Loss is 314.293331861496\n",
            "epoch 345: Loss is 314.2586364746094\n",
            "epoch 346: Loss is 314.2567563056946\n",
            "epoch 347: Loss is 314.24213767051697\n",
            "epoch 348: Loss is 314.22563886642456\n",
            "epoch 349: Loss is 314.2091336250305\n",
            "epoch 350: Loss is 314.1932728290558\n",
            "epoch 351: Loss is 314.1780993938446\n",
            "epoch 352: Loss is 314.1635060310364\n",
            "epoch 353: Loss is 314.1493122577667\n",
            "epoch 354: Loss is 314.1352427005768\n",
            "epoch 355: Loss is 314.12142539024353\n",
            "epoch 356: Loss is 314.1073558330536\n",
            "epoch 357: Loss is 314.093542098999\n",
            "epoch 358: Loss is 314.0807087421417\n",
            "epoch 359: Loss is 314.06728982925415\n",
            "epoch 360: Loss is 314.05440306663513\n",
            "epoch 361: Loss is 314.04189801216125\n",
            "epoch 362: Loss is 314.02948212623596\n",
            "epoch 363: Loss is 314.0169711112976\n",
            "epoch 364: Loss is 314.00416564941406\n",
            "epoch 365: Loss is 313.9917483329773\n",
            "epoch 366: Loss is 313.9797794818878\n",
            "epoch 367: Loss is 313.968546628952\n",
            "epoch 368: Loss is 313.95787477493286\n",
            "epoch 369: Loss is 313.9472622871399\n",
            "epoch 370: Loss is 313.9371120929718\n",
            "epoch 371: Loss is 313.9259548187256\n",
            "epoch 372: Loss is 313.91524052619934\n",
            "epoch 373: Loss is 313.8957087993622\n",
            "epoch 374: Loss is 313.8936171531677\n",
            "epoch 375: Loss is 313.88425612449646\n",
            "epoch 376: Loss is 313.8743894100189\n",
            "epoch 377: Loss is 313.86479568481445\n",
            "epoch 378: Loss is 313.8556752204895\n",
            "epoch 379: Loss is 313.8469204902649\n",
            "epoch 380: Loss is 313.8381519317627\n",
            "epoch 381: Loss is 313.8293619155884\n",
            "epoch 382: Loss is 313.8210105895996\n",
            "epoch 383: Loss is 313.8124899864197\n",
            "epoch 384: Loss is 313.80417251586914\n",
            "epoch 385: Loss is 313.79606342315674\n",
            "epoch 386: Loss is 313.7877321243286\n",
            "epoch 387: Loss is 313.77916073799133\n",
            "epoch 388: Loss is 313.7713460922241\n",
            "epoch 389: Loss is 313.76396083831787\n",
            "epoch 390: Loss is 313.75701427459717\n",
            "epoch 391: Loss is 313.75031328201294\n",
            "epoch 392: Loss is 313.74303817749023\n",
            "epoch 393: Loss is 313.73568892478943\n",
            "epoch 394: Loss is 313.72854685783386\n",
            "epoch 395: Loss is 313.72195315361023\n",
            "epoch 396: Loss is 313.7140965461731\n",
            "epoch 397: Loss is 313.7088258266449\n",
            "epoch 398: Loss is 313.7028663158417\n",
            "epoch 399: Loss is 313.6967968940735\n",
            "epoch 400: Loss is 313.6908264160156\n",
            "epoch 401: Loss is 313.68483090400696\n",
            "epoch 402: Loss is 313.6783673763275\n",
            "epoch 403: Loss is 313.6718008518219\n",
            "epoch 404: Loss is 313.66560077667236\n",
            "epoch 405: Loss is 313.6595182418823\n",
            "epoch 406: Loss is 313.6529288291931\n",
            "epoch 407: Loss is 313.6459467411041\n",
            "epoch 408: Loss is 313.6391410827637\n",
            "epoch 409: Loss is 313.6332187652588\n",
            "epoch 410: Loss is 313.62753415107727\n",
            "epoch 411: Loss is 313.6212058067322\n",
            "epoch 412: Loss is 313.6140193939209\n",
            "epoch 413: Loss is 313.607253074646\n",
            "epoch 414: Loss is 313.60102224349976\n",
            "epoch 415: Loss is 313.59609055519104\n",
            "epoch 416: Loss is 313.58869552612305\n",
            "epoch 417: Loss is 313.5811891555786\n",
            "epoch 418: Loss is 313.574791431427\n",
            "epoch 419: Loss is 313.56713032722473\n",
            "epoch 420: Loss is 313.5607464313507\n",
            "epoch 421: Loss is 313.5544548034668\n",
            "epoch 422: Loss is 313.5469181537628\n",
            "epoch 423: Loss is 313.5386505126953\n",
            "epoch 424: Loss is 313.5319969654083\n",
            "epoch 425: Loss is 313.5258402824402\n",
            "epoch 426: Loss is 313.5179307460785\n",
            "epoch 427: Loss is 313.5094401836395\n",
            "epoch 428: Loss is 313.50198435783386\n",
            "epoch 429: Loss is 313.4956636428833\n",
            "epoch 430: Loss is 313.48652601242065\n",
            "epoch 431: Loss is 313.4775867462158\n",
            "epoch 432: Loss is 313.47183752059937\n",
            "epoch 433: Loss is 313.46551418304443\n",
            "epoch 434: Loss is 313.455935716629\n",
            "epoch 435: Loss is 313.4483006000519\n",
            "epoch 436: Loss is 313.44248032569885\n",
            "epoch 437: Loss is 313.4357192516327\n",
            "epoch 438: Loss is 313.4271206855774\n",
            "epoch 439: Loss is 313.42015075683594\n",
            "epoch 440: Loss is 313.4138355255127\n",
            "epoch 441: Loss is 313.4060778617859\n",
            "epoch 442: Loss is 313.3974361419678\n",
            "epoch 443: Loss is 313.3899931907654\n",
            "epoch 444: Loss is 313.3833770751953\n",
            "epoch 445: Loss is 313.37608218193054\n",
            "epoch 446: Loss is 313.36744952201843\n",
            "epoch 447: Loss is 313.3593559265137\n",
            "epoch 448: Loss is 313.35150504112244\n",
            "epoch 449: Loss is 313.34262323379517\n",
            "epoch 450: Loss is 313.33391427993774\n",
            "epoch 451: Loss is 313.32677841186523\n",
            "epoch 452: Loss is 313.3185133934021\n",
            "epoch 453: Loss is 313.30793714523315\n",
            "epoch 454: Loss is 313.2988386154175\n",
            "epoch 455: Loss is 313.2915196418762\n",
            "epoch 456: Loss is 313.2827317714691\n",
            "epoch 457: Loss is 313.2724757194519\n",
            "epoch 458: Loss is 313.26456212997437\n",
            "epoch 459: Loss is 313.2568371295929\n",
            "epoch 460: Loss is 313.24707555770874\n",
            "epoch 461: Loss is 313.2388575077057\n",
            "epoch 462: Loss is 313.23219442367554\n",
            "epoch 463: Loss is 313.2235858440399\n",
            "epoch 464: Loss is 313.2138760089874\n",
            "epoch 465: Loss is 313.2069764137268\n",
            "epoch 466: Loss is 313.200074672699\n",
            "epoch 467: Loss is 313.19108748435974\n",
            "epoch 468: Loss is 313.1825647354126\n",
            "epoch 469: Loss is 313.17491912841797\n",
            "epoch 470: Loss is 313.1666831970215\n",
            "epoch 471: Loss is 313.1587643623352\n",
            "epoch 472: Loss is 313.1518063545227\n",
            "epoch 473: Loss is 313.1438090801239\n",
            "epoch 474: Loss is 313.13467478752136\n",
            "epoch 475: Loss is 313.1270160675049\n",
            "epoch 476: Loss is 313.1196005344391\n",
            "epoch 477: Loss is 313.1109848022461\n",
            "epoch 478: Loss is 313.10362815856934\n",
            "epoch 479: Loss is 313.0965213775635\n",
            "epoch 480: Loss is 313.08845043182373\n",
            "epoch 481: Loss is 313.08096623420715\n",
            "epoch 482: Loss is 313.0743474960327\n",
            "epoch 483: Loss is 313.0661220550537\n",
            "epoch 484: Loss is 313.0588524341583\n",
            "epoch 485: Loss is 313.05348229408264\n",
            "epoch 486: Loss is 313.0467734336853\n",
            "epoch 487: Loss is 313.0396304130554\n",
            "epoch 488: Loss is 313.033563375473\n",
            "epoch 489: Loss is 313.0269591808319\n",
            "epoch 490: Loss is 313.01941108703613\n",
            "epoch 491: Loss is 313.0135941505432\n",
            "epoch 492: Loss is 313.0073719024658\n",
            "epoch 493: Loss is 313.0008409023285\n",
            "epoch 494: Loss is 312.9949872493744\n",
            "epoch 495: Loss is 312.9886190891266\n",
            "epoch 496: Loss is 312.9816298484802\n",
            "epoch 497: Loss is 312.9748704433441\n",
            "epoch 498: Loss is 312.96772027015686\n",
            "epoch 499: Loss is 312.9608039855957\n",
            "epoch 500: Loss is 312.9554808139801\n",
            "epoch 501: Loss is 312.94889402389526\n",
            "epoch 502: Loss is 312.9419400691986\n",
            "epoch 503: Loss is 312.93568420410156\n",
            "epoch 504: Loss is 312.92810583114624\n",
            "epoch 505: Loss is 312.9216260910034\n",
            "epoch 506: Loss is 312.9154164791107\n",
            "epoch 507: Loss is 312.9087452888489\n",
            "epoch 508: Loss is 312.9036531448364\n",
            "epoch 509: Loss is 312.89818143844604\n",
            "epoch 510: Loss is 312.89181637763977\n",
            "epoch 511: Loss is 312.8865694999695\n",
            "epoch 512: Loss is 312.8802161216736\n",
            "epoch 513: Loss is 312.87377548217773\n",
            "epoch 514: Loss is 312.86826276779175\n",
            "epoch 515: Loss is 312.861275434494\n",
            "epoch 516: Loss is 312.85610008239746\n",
            "epoch 517: Loss is 312.8508789539337\n",
            "epoch 518: Loss is 312.8452937602997\n",
            "epoch 519: Loss is 312.8395428657532\n",
            "epoch 520: Loss is 312.83259773254395\n",
            "epoch 521: Loss is 312.8263506889343\n",
            "epoch 522: Loss is 312.8214304447174\n",
            "epoch 523: Loss is 312.81562638282776\n",
            "epoch 524: Loss is 312.8116579055786\n",
            "epoch 525: Loss is 312.80606150627136\n",
            "epoch 526: Loss is 312.8002824783325\n",
            "epoch 527: Loss is 312.7953279018402\n",
            "epoch 528: Loss is 312.7884724140167\n",
            "epoch 529: Loss is 312.78554916381836\n",
            "epoch 530: Loss is 312.7774693965912\n",
            "epoch 531: Loss is 312.77064204216003\n",
            "epoch 532: Loss is 312.76625752449036\n",
            "epoch 533: Loss is 312.7609763145447\n",
            "epoch 534: Loss is 312.75656151771545\n",
            "epoch 535: Loss is 312.75084471702576\n",
            "epoch 536: Loss is 312.7450830936432\n",
            "epoch 537: Loss is 312.7399067878723\n",
            "epoch 538: Loss is 312.7342526912689\n",
            "epoch 539: Loss is 312.7291855812073\n",
            "epoch 540: Loss is 312.72346353530884\n",
            "epoch 541: Loss is 312.718154668808\n",
            "epoch 542: Loss is 312.7138147354126\n",
            "epoch 543: Loss is 312.7697205543518\n",
            "epoch 544: Loss is 312.7743721008301\n",
            "epoch 545: Loss is 312.69518852233887\n",
            "epoch 546: Loss is 312.6909227371216\n",
            "epoch 547: Loss is 312.6859815120697\n",
            "epoch 548: Loss is 312.6802260875702\n",
            "epoch 549: Loss is 312.6754171848297\n",
            "epoch 550: Loss is 312.6700406074524\n",
            "epoch 551: Loss is 312.66459798812866\n",
            "epoch 552: Loss is 312.65948128700256\n",
            "epoch 553: Loss is 312.65392541885376\n",
            "epoch 554: Loss is 312.64878606796265\n",
            "epoch 555: Loss is 312.6437454223633\n",
            "epoch 556: Loss is 312.6386499404907\n",
            "epoch 557: Loss is 312.63412261009216\n",
            "epoch 558: Loss is 312.6294343471527\n",
            "epoch 559: Loss is 312.6252224445343\n",
            "epoch 560: Loss is 312.6208951473236\n",
            "epoch 561: Loss is 312.6162624359131\n",
            "epoch 562: Loss is 312.61157393455505\n",
            "epoch 563: Loss is 312.6063208580017\n",
            "epoch 564: Loss is 312.60229659080505\n",
            "epoch 565: Loss is 312.59794306755066\n",
            "epoch 566: Loss is 312.59421253204346\n",
            "epoch 567: Loss is 312.5897886753082\n",
            "epoch 568: Loss is 312.5848195552826\n",
            "epoch 569: Loss is 312.5818929672241\n",
            "epoch 570: Loss is 312.5772936344147\n",
            "epoch 571: Loss is 312.5744206905365\n",
            "epoch 572: Loss is 312.57131934165955\n",
            "epoch 573: Loss is 312.5680193901062\n",
            "epoch 574: Loss is 312.5654842853546\n",
            "epoch 575: Loss is 312.5619695186615\n",
            "epoch 576: Loss is 312.5582036972046\n",
            "epoch 577: Loss is 312.5545437335968\n",
            "epoch 578: Loss is 312.55139541625977\n",
            "epoch 579: Loss is 312.5489025115967\n",
            "epoch 580: Loss is 312.54604291915894\n",
            "epoch 581: Loss is 312.54315423965454\n",
            "epoch 582: Loss is 312.54032373428345\n",
            "epoch 583: Loss is 312.5374915599823\n",
            "epoch 584: Loss is 312.53512358665466\n",
            "epoch 585: Loss is 312.5312943458557\n",
            "epoch 586: Loss is 312.5282971858978\n",
            "epoch 587: Loss is 312.5244290828705\n",
            "epoch 588: Loss is 312.55663108825684\n",
            "epoch 589: Loss is 312.5411615371704\n",
            "epoch 590: Loss is 312.51753425598145\n",
            "epoch 591: Loss is 312.5126132965088\n",
            "epoch 592: Loss is 312.5088212490082\n",
            "epoch 593: Loss is 312.5061285495758\n",
            "epoch 594: Loss is 312.50426483154297\n",
            "epoch 595: Loss is 312.50224566459656\n",
            "epoch 596: Loss is 312.500625371933\n",
            "epoch 597: Loss is 312.49800157546997\n",
            "epoch 598: Loss is 312.4956097602844\n",
            "epoch 599: Loss is 312.49291729927063\n",
            "epoch 600: Loss is 312.49039602279663\n",
            "epoch 601: Loss is 312.4878087043762\n",
            "epoch 602: Loss is 312.4846339225769\n",
            "epoch 603: Loss is 312.481641292572\n",
            "epoch 604: Loss is 312.4779055118561\n",
            "epoch 605: Loss is 312.4743049144745\n",
            "epoch 606: Loss is 312.47041368484497\n",
            "epoch 607: Loss is 312.46806812286377\n",
            "epoch 608: Loss is 312.4646577835083\n",
            "epoch 609: Loss is 312.4619870185852\n",
            "epoch 610: Loss is 312.4586534500122\n",
            "epoch 611: Loss is 312.45632433891296\n",
            "epoch 612: Loss is 312.4542486667633\n",
            "epoch 613: Loss is 312.4514284133911\n",
            "epoch 614: Loss is 312.4484782218933\n",
            "epoch 615: Loss is 312.44410276412964\n",
            "epoch 616: Loss is 312.4402358531952\n",
            "epoch 617: Loss is 312.4363822937012\n",
            "epoch 618: Loss is 312.43354201316833\n",
            "epoch 619: Loss is 312.4308171272278\n",
            "epoch 620: Loss is 312.4280068874359\n",
            "epoch 621: Loss is 312.4252552986145\n",
            "epoch 622: Loss is 312.4226498603821\n",
            "epoch 623: Loss is 312.4202632904053\n",
            "epoch 624: Loss is 312.41755175590515\n",
            "epoch 625: Loss is 312.4143223762512\n",
            "epoch 626: Loss is 312.4103217124939\n",
            "epoch 627: Loss is 312.40632605552673\n",
            "epoch 628: Loss is 312.4019191265106\n",
            "epoch 629: Loss is 312.39775109291077\n",
            "epoch 630: Loss is 312.3933289051056\n",
            "epoch 631: Loss is 312.38975715637207\n",
            "epoch 632: Loss is 312.3858494758606\n",
            "epoch 633: Loss is 312.37595224380493\n",
            "epoch 634: Loss is 312.37822794914246\n",
            "epoch 635: Loss is 312.37526679039\n",
            "epoch 636: Loss is 312.3707957267761\n",
            "epoch 637: Loss is 312.3674244880676\n",
            "epoch 638: Loss is 312.3638119697571\n",
            "epoch 639: Loss is 312.36173725128174\n",
            "epoch 640: Loss is 312.35909152030945\n",
            "epoch 641: Loss is 312.357373714447\n",
            "epoch 642: Loss is 312.3547275066376\n",
            "epoch 643: Loss is 312.3525369167328\n",
            "epoch 644: Loss is 312.3494772911072\n",
            "epoch 645: Loss is 312.34644842147827\n",
            "epoch 646: Loss is 312.3426306247711\n",
            "epoch 647: Loss is 312.3389434814453\n",
            "epoch 648: Loss is 312.33439803123474\n",
            "epoch 649: Loss is 312.32998919487\n",
            "epoch 650: Loss is 312.3247902393341\n",
            "epoch 651: Loss is 312.32033348083496\n",
            "epoch 652: Loss is 312.31530141830444\n",
            "epoch 653: Loss is 312.3124439716339\n",
            "epoch 654: Loss is 312.3075442314148\n",
            "epoch 655: Loss is 312.30473470687866\n",
            "epoch 656: Loss is 312.30027079582214\n",
            "epoch 657: Loss is 312.29794549942017\n",
            "epoch 658: Loss is 312.2957420349121\n",
            "epoch 659: Loss is 312.2949719429016\n",
            "epoch 660: Loss is 312.29356241226196\n",
            "epoch 661: Loss is 312.29220485687256\n",
            "epoch 662: Loss is 312.2893416881561\n",
            "epoch 663: Loss is 312.2864394187927\n",
            "epoch 664: Loss is 312.28173780441284\n",
            "epoch 665: Loss is 312.27652955055237\n",
            "epoch 666: Loss is 312.27109456062317\n",
            "epoch 667: Loss is 312.2663896083832\n",
            "epoch 668: Loss is 312.2616105079651\n",
            "epoch 669: Loss is 312.25644302368164\n",
            "epoch 670: Loss is 312.2511510848999\n",
            "epoch 671: Loss is 312.2464280128479\n",
            "epoch 672: Loss is 312.2422432899475\n",
            "epoch 673: Loss is 312.238582611084\n",
            "epoch 674: Loss is 312.23470163345337\n",
            "epoch 675: Loss is 312.2303431034088\n",
            "epoch 676: Loss is 312.2257423400879\n",
            "epoch 677: Loss is 312.2226405143738\n",
            "epoch 678: Loss is 312.2183494567871\n",
            "epoch 679: Loss is 312.21505975723267\n",
            "epoch 680: Loss is 312.20972299575806\n",
            "epoch 681: Loss is 312.2049973011017\n",
            "epoch 682: Loss is 312.2005181312561\n",
            "epoch 683: Loss is 312.1973626613617\n",
            "epoch 684: Loss is 312.1928701400757\n",
            "epoch 685: Loss is 312.1889817714691\n",
            "epoch 686: Loss is 312.18563055992126\n",
            "epoch 687: Loss is 312.18268871307373\n",
            "epoch 688: Loss is 312.1796591281891\n",
            "epoch 689: Loss is 312.17621755599976\n",
            "epoch 690: Loss is 312.1724615097046\n",
            "epoch 691: Loss is 312.1689672470093\n",
            "epoch 692: Loss is 312.16571593284607\n",
            "epoch 693: Loss is 312.16232466697693\n",
            "epoch 694: Loss is 312.1585009098053\n",
            "epoch 695: Loss is 312.1542820930481\n",
            "epoch 696: Loss is 312.15010166168213\n",
            "epoch 697: Loss is 312.14515447616577\n",
            "epoch 698: Loss is 312.140177488327\n",
            "epoch 699: Loss is 312.13459968566895\n",
            "epoch 700: Loss is 312.1299452781677\n",
            "epoch 701: Loss is 312.12511920928955\n",
            "epoch 702: Loss is 312.1212658882141\n",
            "epoch 703: Loss is 312.11778688430786\n",
            "epoch 704: Loss is 312.11345744132996\n",
            "epoch 705: Loss is 312.109299659729\n",
            "epoch 706: Loss is 312.105021238327\n",
            "epoch 707: Loss is 312.10148763656616\n",
            "epoch 708: Loss is 312.09871339797974\n",
            "epoch 709: Loss is 312.09686374664307\n",
            "epoch 710: Loss is 312.09579277038574\n",
            "epoch 711: Loss is 312.0947344303131\n",
            "epoch 712: Loss is 312.0932414531708\n",
            "epoch 713: Loss is 312.0907461643219\n",
            "epoch 714: Loss is 312.0873849391937\n",
            "epoch 715: Loss is 312.08342599868774\n",
            "epoch 716: Loss is 312.0803933143616\n",
            "epoch 717: Loss is 312.07812452316284\n",
            "epoch 718: Loss is 312.07696557044983\n",
            "epoch 719: Loss is 312.07647132873535\n",
            "epoch 720: Loss is 312.07581663131714\n",
            "epoch 721: Loss is 312.0739281177521\n",
            "epoch 722: Loss is 312.0707223415375\n",
            "epoch 723: Loss is 312.0665831565857\n",
            "epoch 724: Loss is 312.0624041557312\n",
            "epoch 725: Loss is 312.0581750869751\n",
            "epoch 726: Loss is 312.0540828704834\n",
            "epoch 727: Loss is 312.0498836040497\n",
            "epoch 728: Loss is 312.04565048217773\n",
            "epoch 729: Loss is 312.04208040237427\n",
            "epoch 730: Loss is 312.0385525226593\n",
            "epoch 731: Loss is 312.0363109111786\n",
            "epoch 732: Loss is 312.03302907943726\n",
            "epoch 733: Loss is 312.03051805496216\n",
            "epoch 734: Loss is 312.0273241996765\n",
            "epoch 735: Loss is 312.0257260799408\n",
            "epoch 736: Loss is 312.02462935447693\n",
            "epoch 737: Loss is 312.02441477775574\n",
            "epoch 738: Loss is 312.0243601799011\n",
            "epoch 739: Loss is 312.0241370201111\n",
            "epoch 740: Loss is 312.02320671081543\n",
            "epoch 741: Loss is 312.0215549468994\n",
            "epoch 742: Loss is 312.0192234516144\n",
            "epoch 743: Loss is 312.0161454677582\n",
            "epoch 744: Loss is 312.01243829727173\n",
            "epoch 745: Loss is 312.0083677768707\n",
            "epoch 746: Loss is 312.00467801094055\n",
            "epoch 747: Loss is 312.00145983695984\n",
            "epoch 748: Loss is 311.99869775772095\n",
            "epoch 749: Loss is 311.99623703956604\n",
            "epoch 750: Loss is 311.9940004348755\n",
            "epoch 751: Loss is 311.99132919311523\n",
            "epoch 752: Loss is 311.98878359794617\n",
            "epoch 753: Loss is 311.98587012290955\n",
            "epoch 754: Loss is 311.98270201683044\n",
            "epoch 755: Loss is 311.9785828590393\n",
            "epoch 756: Loss is 311.97468972206116\n",
            "epoch 757: Loss is 311.9714243412018\n",
            "epoch 758: Loss is 311.9688334465027\n",
            "epoch 759: Loss is 311.96732783317566\n",
            "epoch 760: Loss is 311.9658272266388\n",
            "epoch 761: Loss is 311.96475410461426\n",
            "epoch 762: Loss is 311.96347093582153\n",
            "epoch 763: Loss is 311.9621684551239\n",
            "epoch 764: Loss is 311.9613506793976\n",
            "epoch 765: Loss is 311.9607813358307\n",
            "epoch 766: Loss is 311.9607527256012\n",
            "epoch 767: Loss is 311.9601767063141\n",
            "epoch 768: Loss is 311.958580493927\n",
            "epoch 769: Loss is 311.9563684463501\n",
            "epoch 770: Loss is 311.95404505729675\n",
            "epoch 771: Loss is 311.95225501060486\n",
            "epoch 772: Loss is 311.9505829811096\n",
            "epoch 773: Loss is 311.9492781162262\n",
            "epoch 774: Loss is 311.9483380317688\n",
            "epoch 775: Loss is 311.94716024398804\n",
            "epoch 776: Loss is 311.9459488391876\n",
            "epoch 777: Loss is 311.94447112083435\n",
            "epoch 778: Loss is 311.94314193725586\n",
            "epoch 779: Loss is 311.9417791366577\n",
            "epoch 780: Loss is 311.94033455848694\n",
            "epoch 781: Loss is 311.9395122528076\n",
            "epoch 782: Loss is 311.9386920928955\n",
            "epoch 783: Loss is 311.93809270858765\n",
            "epoch 784: Loss is 311.93695878982544\n",
            "epoch 785: Loss is 311.9354066848755\n",
            "epoch 786: Loss is 311.93357515335083\n",
            "epoch 787: Loss is 311.93118929862976\n",
            "epoch 788: Loss is 311.928995847702\n",
            "epoch 789: Loss is 311.9266035556793\n",
            "epoch 790: Loss is 311.9245753288269\n",
            "epoch 791: Loss is 311.9229025840759\n",
            "epoch 792: Loss is 311.9213852882385\n",
            "epoch 793: Loss is 311.9209382534027\n",
            "epoch 794: Loss is 311.91989374160767\n",
            "epoch 795: Loss is 311.9191265106201\n",
            "epoch 796: Loss is 311.9179241657257\n",
            "epoch 797: Loss is 311.91940331459045\n",
            "epoch 798: Loss is 311.92048740386963\n",
            "epoch 799: Loss is 311.92194533348083\n",
            "epoch 800: Loss is 311.92300152778625\n",
            "epoch 801: Loss is 311.9250614643097\n",
            "epoch 802: Loss is 311.9257004261017\n",
            "epoch 803: Loss is 311.9251117706299\n",
            "epoch 804: Loss is 311.9239413738251\n",
            "epoch 805: Loss is 311.92120695114136\n",
            "epoch 806: Loss is 311.9179997444153\n",
            "epoch 807: Loss is 311.91436862945557\n",
            "epoch 808: Loss is 311.9105715751648\n",
            "epoch 809: Loss is 311.90723609924316\n",
            "epoch 810: Loss is 311.90418553352356\n",
            "epoch 811: Loss is 311.9010031223297\n",
            "epoch 812: Loss is 311.89745688438416\n",
            "epoch 813: Loss is 311.89321970939636\n",
            "epoch 814: Loss is 311.8888602256775\n",
            "epoch 815: Loss is 311.8847486972809\n",
            "epoch 816: Loss is 311.8810794353485\n",
            "epoch 817: Loss is 311.8780047893524\n",
            "epoch 818: Loss is 311.875372171402\n",
            "epoch 819: Loss is 311.8737015724182\n",
            "epoch 820: Loss is 311.8726613521576\n",
            "epoch 821: Loss is 311.8719618320465\n",
            "epoch 822: Loss is 311.8720164299011\n",
            "epoch 823: Loss is 311.87192964553833\n",
            "epoch 824: Loss is 311.87252402305603\n",
            "epoch 825: Loss is 311.8737826347351\n",
            "epoch 826: Loss is 311.874449968338\n",
            "epoch 827: Loss is 311.8760709762573\n",
            "epoch 828: Loss is 311.87795996665955\n",
            "epoch 829: Loss is 311.8796796798706\n",
            "epoch 830: Loss is 311.8813724517822\n",
            "epoch 831: Loss is 311.8824121952057\n",
            "epoch 832: Loss is 311.8835496902466\n",
            "epoch 833: Loss is 311.88445496559143\n",
            "epoch 834: Loss is 311.8844950199127\n",
            "epoch 836: Loss is 311.88405418395996\n",
            "epoch 837: Loss is 311.88319754600525\n",
            "epoch 838: Loss is 311.88165640830994\n",
            "epoch 839: Loss is 311.87961053848267\n",
            "epoch 840: Loss is 311.87723183631897\n",
            "epoch 841: Loss is 311.8744831085205\n",
            "epoch 842: Loss is 311.8712434768677\n",
            "epoch 843: Loss is 311.86756777763367\n",
            "epoch 844: Loss is 311.8637981414795\n",
            "epoch 845: Loss is 311.85974383354187\n",
            "epoch 846: Loss is 311.8557412624359\n",
            "epoch 847: Loss is 311.8517825603485\n",
            "epoch 848: Loss is 311.8481283187866\n",
            "epoch 849: Loss is 311.84487795829773\n",
            "epoch 850: Loss is 311.8417024612427\n",
            "epoch 851: Loss is 311.8388338088989\n",
            "epoch 852: Loss is 311.83677649497986\n",
            "epoch 853: Loss is 311.8354630470276\n",
            "epoch 854: Loss is 311.8348059654236\n",
            "epoch 855: Loss is 311.83465361595154\n",
            "epoch 856: Loss is 311.8351285457611\n",
            "epoch 857: Loss is 311.83624601364136\n",
            "epoch 858: Loss is 311.8380038738251\n",
            "epoch 859: Loss is 311.84029483795166\n",
            "epoch 860: Loss is 311.84235668182373\n",
            "epoch 861: Loss is 311.84471368789673\n",
            "epoch 862: Loss is 311.84666776657104\n",
            "epoch 863: Loss is 311.85052585601807\n",
            "epoch 864: Loss is 311.85550451278687\n",
            "epoch 865: Loss is 311.85853242874146\n",
            "epoch 866: Loss is 311.8617916107178\n",
            "epoch 867: Loss is 311.86516857147217\n",
            "epoch 868: Loss is 311.86865854263306\n",
            "epoch 869: Loss is 311.8726181983948\n",
            "epoch 870: Loss is 311.87584805488586\n",
            "epoch 871: Loss is 311.8780653476715\n",
            "epoch 872: Loss is 311.87940406799316\n",
            "epoch 873: Loss is 311.8792214393616\n",
            "epoch 874: Loss is 311.8773169517517\n",
            "epoch 875: Loss is 311.8746871948242\n",
            "epoch 876: Loss is 311.8722081184387\n",
            "epoch 877: Loss is 311.86987495422363\n",
            "epoch 878: Loss is 311.86764883995056\n",
            "epoch 879: Loss is 311.8652057647705\n",
            "epoch 880: Loss is 311.86224365234375\n",
            "epoch 881: Loss is 311.85906982421875\n",
            "epoch 882: Loss is 311.8562822341919\n",
            "epoch 883: Loss is 311.85360193252563\n",
            "epoch 884: Loss is 311.8508973121643\n",
            "epoch 885: Loss is 311.8490147590637\n",
            "epoch 886: Loss is 311.84783005714417\n",
            "epoch 887: Loss is 311.8466041088104\n",
            "epoch 888: Loss is 311.8459038734436\n",
            "epoch 889: Loss is 311.84561133384705\n",
            "epoch 890: Loss is 311.8446743488312\n",
            "epoch 891: Loss is 311.84462809562683\n",
            "epoch 892: Loss is 311.8453793525696\n",
            "epoch 893: Loss is 311.8449218273163\n",
            "epoch 894: Loss is 311.84422612190247\n",
            "epoch 895: Loss is 311.8435523509979\n",
            "epoch 896: Loss is 311.84206891059875\n",
            "epoch 897: Loss is 311.84091234207153\n",
            "epoch 898: Loss is 311.8404562473297\n",
            "epoch 899: Loss is 311.83973956108093\n",
            "epoch 900: Loss is 311.8385627269745\n",
            "epoch 901: Loss is 311.8367998600006\n",
            "epoch 902: Loss is 311.83439016342163\n",
            "epoch 903: Loss is 311.83224630355835\n",
            "epoch 904: Loss is 311.8304741382599\n",
            "epoch 905: Loss is 311.8283808231354\n",
            "epoch 906: Loss is 311.8263657093048\n",
            "epoch 907: Loss is 311.82430815696716\n",
            "epoch 908: Loss is 311.82250809669495\n",
            "epoch 909: Loss is 311.8216116428375\n",
            "epoch 910: Loss is 311.8209581375122\n",
            "epoch 911: Loss is 311.82023310661316\n",
            "epoch 912: Loss is 311.8196210861206\n",
            "epoch 913: Loss is 311.8185317516327\n",
            "epoch 914: Loss is 311.8164472579956\n",
            "epoch 915: Loss is 311.8138270378113\n",
            "epoch 916: Loss is 311.8108615875244\n",
            "epoch 917: Loss is 311.80785870552063\n",
            "epoch 918: Loss is 311.805428981781\n",
            "epoch 919: Loss is 311.8034944534302\n",
            "epoch 920: Loss is 311.8018252849579\n",
            "epoch 921: Loss is 311.80007338523865\n",
            "epoch 922: Loss is 311.7983434200287\n",
            "epoch 923: Loss is 311.79643964767456\n",
            "epoch 924: Loss is 311.7941334247589\n",
            "epoch 925: Loss is 311.79221844673157\n",
            "epoch 926: Loss is 311.79111337661743\n",
            "epoch 927: Loss is 311.79044675827026\n",
            "epoch 928: Loss is 311.78986954689026\n",
            "epoch 929: Loss is 311.7889349460602\n",
            "epoch 930: Loss is 311.7879295349121\n",
            "epoch 931: Loss is 311.78738379478455\n",
            "epoch 932: Loss is 311.78689908981323\n",
            "epoch 933: Loss is 311.7858603000641\n",
            "epoch 934: Loss is 311.7844834327698\n",
            "epoch 935: Loss is 311.78289794921875\n",
            "epoch 936: Loss is 311.78067350387573\n",
            "epoch 937: Loss is 311.77812814712524\n",
            "epoch 938: Loss is 311.7754662036896\n",
            "epoch 939: Loss is 311.773380279541\n",
            "epoch 940: Loss is 311.7720847129822\n",
            "epoch 941: Loss is 311.7710897922516\n",
            "epoch 942: Loss is 311.77061676979065\n",
            "epoch 943: Loss is 311.7706115245819\n",
            "epoch 944: Loss is 311.7706332206726\n",
            "epoch 945: Loss is 311.7705171108246\n",
            "epoch 946: Loss is 311.769903421402\n",
            "epoch 947: Loss is 311.76971793174744\n",
            "epoch 948: Loss is 311.7697355747223\n",
            "epoch 949: Loss is 311.7696294784546\n",
            "epoch 950: Loss is 311.7689492702484\n",
            "epoch 951: Loss is 311.76787066459656\n",
            "epoch 952: Loss is 311.7667062282562\n",
            "epoch 953: Loss is 311.76570987701416\n",
            "epoch 954: Loss is 311.76514196395874\n",
            "epoch 955: Loss is 311.76493763923645\n",
            "epoch 956: Loss is 311.7647671699524\n",
            "epoch 957: Loss is 311.76445722579956\n",
            "epoch 958: Loss is 311.76397681236267\n",
            "epoch 959: Loss is 311.76315927505493\n",
            "epoch 960: Loss is 311.7619323730469\n",
            "epoch 961: Loss is 311.76059436798096\n",
            "epoch 962: Loss is 311.75914430618286\n",
            "epoch 963: Loss is 311.75792717933655\n",
            "epoch 964: Loss is 311.7571723461151\n",
            "epoch 965: Loss is 311.7560701370239\n",
            "epoch 966: Loss is 311.7551748752594\n",
            "epoch 967: Loss is 311.7537932395935\n",
            "epoch 968: Loss is 311.75252199172974\n",
            "epoch 969: Loss is 311.7521708011627\n",
            "epoch 970: Loss is 311.75241136550903\n",
            "epoch 971: Loss is 311.753378868103\n",
            "epoch 972: Loss is 311.7547025680542\n",
            "epoch 973: Loss is 311.7561502456665\n",
            "epoch 974: Loss is 311.7574620246887\n",
            "epoch 975: Loss is 311.75830149650574\n",
            "epoch 976: Loss is 311.7585988044739\n",
            "epoch 977: Loss is 311.7585594654083\n",
            "epoch 978: Loss is 311.7582468986511\n",
            "epoch 979: Loss is 311.7575991153717\n",
            "epoch 980: Loss is 311.75759267807007\n",
            "epoch 981: Loss is 311.7574005126953\n",
            "epoch 982: Loss is 311.75842118263245\n",
            "epoch 983: Loss is 311.7594404220581\n",
            "epoch 984: Loss is 311.7615211009979\n",
            "epoch 985: Loss is 311.7640700340271\n",
            "epoch 986: Loss is 311.76596879959106\n",
            "epoch 987: Loss is 311.76738238334656\n",
            "epoch 988: Loss is 311.76868867874146\n",
            "epoch 989: Loss is 311.76991152763367\n",
            "epoch 990: Loss is 311.770676612854\n",
            "epoch 991: Loss is 311.77073526382446\n",
            "epoch 992: Loss is 311.7700333595276\n",
            "epoch 993: Loss is 311.7685282230377\n",
            "epoch 994: Loss is 311.7664260864258\n",
            "epoch 995: Loss is 311.764164686203\n",
            "epoch 996: Loss is 311.7619740962982\n",
            "epoch 997: Loss is 311.7599914073944\n",
            "epoch 998: Loss is 311.75843954086304\n",
            "epoch 999: Loss is 311.7576766014099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the loss after training"
      ],
      "metadata": {
        "id": "TmfJg62-Ahp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epoch_loss_list, label =\"Epoch Loss\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig('/content/drive/MyDrive/BIOSCAN/model_checkpoints_new/loss_per_epoch.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K4v-JAdIWzXg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "dc412d50-6004-4ae9-be66-f4cd860e9116"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxuQbqskaDkJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}