{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# requirements"],"metadata":{"id":"EN3EsTEzIUcZ"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import scipy.io as sio\n","import numpy as np\n","import random\n","from random import randrange, shuffle, random, randint\n","\n","import math\n","import torch\n","\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import pickle\n","import matplotlib.pyplot as plt"],"metadata":{"id":"AueLRah0IT-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load data and tokenize"],"metadata":{"id":"1lURUvE3HuCX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7F8QcTd5HV6a"},"outputs":[],"source":["class PabloDNADataset:\n","    def __init__(self, file_path):\n","        self.df = pd.read_csv(file_path, sep=\"\\t\", encoding=\"unicode_escape\")\n","\n","    def clean_nan(self, col_names, replace_orig=False):\n","        clean_df = self.df.dropna(subset=col_names)\n","        clean_df = clean_df.reset_index(drop=True)\n","        if replace_orig:\n","            self.df = clean_df\n","        return clean_df\n","    \n","    def change_RXY2N(self, col_names, replace_orig=False):\n","      full_pattern = re.compile('[^ACGTN\\-]') \n","      self.df[col_names] = self.df[col_names].apply(lambda x: re.sub(full_pattern, 'N', x))\n","      # if replace_orig:\n","      #   self.df[col_names] = clean_nucleotides\n","      # return clean_str_df\n","\n","\n","    def generate_mini_sample(self, dataframe=None, bin_count=20, output_path=\"mini_sample.tsv\"):\n","        if dataframe is None:\n","            dataframe = self.df\n","        bins = list(dataframe['bin_uri'].unique())\n","        rd1 = random.sample(range(0, len(bins)), bin_count)\n","        bins = [bins[i] for i in rd1]\n","        mini_df = dataframe.loc[dataframe['bin_uri'].isin(bins)]\n","        mini_df = mini_df.reset_index(drop=True)\n","        # mini_df = dataframe.iloc[0:sample_count]\n","        # mini_df = dataframe.take(np.random.permutation(len(dataframe))[:sample_count])\n","        mini_df.to_csv(output_path, sep=\"\\t\")\n","\n","    def get_info(self, dataframe=None):\n","        if dataframe is None:\n","            dataframe = self.df\n","        print(\"Total data: \", len(dataframe))\n","        print(\"Number of bin clusters: \", len(dataframe['bin_uri'].unique()))\n","\n"]},{"cell_type":"code","source":["def tokenizer(dna_sentence, k_mer_dict, k_mer_length, stride=1):\n","    tokens = []\n","    for i in range(0, len(dna_sentence) - k_mer_length + 1, stride):\n","        k_mer = dna_sentence[i:i + k_mer_length]\n","        tokens.append(k_mer_dict[k_mer])\n","    return tokens\n","\n","\n","\n","class SampleDNAData(Dataset):\n","    \"\"\"Barcode Dataset\"\"\"\n","\n","    @staticmethod\n","    def get_all_kmers(k_mer_length, alphabet=None) -> list:\n","        \"\"\"\n","        :rtype: object\n","        \"\"\"\n","\n","        def base_convert(num, base, length):\n","            result = []\n","            while num > 0:\n","                result.insert(0, num % base)\n","                num = num // base\n","            while len(result) < length:\n","                result.insert(0, 0)\n","            return result\n","\n","        if alphabet is None:\n","            alphabet = [\"A\", \"C\", \"G\", \"T\", \"-\", \"N\"]\n","        k_mer_counts = len(alphabet) ** k_mer_length\n","        all_k_mers_list = []\n","        for i in range(k_mer_counts):\n","            code = base_convert(num=i, base=len(alphabet), length=k_mer_length)\n","            k_mer = \"\"\n","            for j in range(k_mer_length):\n","                k_mer += alphabet[code[j]]\n","            all_k_mers_list.append(k_mer)\n","\n","        return all_k_mers_list\n","\n","    def __init__(self, file_path, k_mer=4, data_count=8, max_mask_count=5, max_len=256):\n","        pablo_dataset = PabloDNADataset(file_path)\n","        # for removing X,R,Y letters from data\n","        pablo_dataset.change_RXY2N(\"nucleotides\")\n","        self.dna_nucleotides = list(pablo_dataset.df[\"nucleotides\"].values)\n","        word_list = SampleDNAData.get_all_kmers(k_mer)\n","\n","        word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n","        for i, w in enumerate(word_list):\n","            word_dict[w] = i + 4\n","            number_dict = {i: w for i, w in enumerate(word_dict)}\n","\n","        self.word_dict = word_dict\n","        self.number_dict = number_dict\n","\n","        self.vocab_size = len(word_dict)\n","        self.max_len = max_len\n","\n","        self.batch = []\n","        positive = negative = 0\n","        while positive != data_count / 2 or negative != data_count / 2:\n","            is_positive = randrange(0, 2)\n","\n","            tokens_a_index, tokens_b_index = 0, 0\n","            while tokens_a_index == tokens_b_index:\n","                tokens_a_index, tokens_b_index = randrange(len(self.dna_nucleotides)), randrange(\n","                    len(self.dna_nucleotides))\n","\n","            if is_positive:\n","                dna_a = self.dna_nucleotides[tokens_a_index]\n","                dna_b = dna_a\n","            else:\n","                dna_a = self.dna_nucleotides[tokens_a_index]\n","                dna_b = self.dna_nucleotides[tokens_b_index]\n","\n","            rand_len = randrange(128, 256)\n","\n","            dna_a = dna_a[0:len(dna_a) // 2][0:rand_len]  # max_len//2 - 3]\n","            dna_b = dna_b[len(dna_b) // 2:][0:rand_len]  # max_len//2 - 3]\n","            tokens_a = tokenizer(dna_a, word_dict, k_mer, stride=1)\n","            tokens_b = tokenizer(dna_b, word_dict, k_mer, stride=1)\n","            input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n","            segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n","\n","            # MASK LM\n","            n_pred = min(max_mask_count, max(1, int(round(len(input_ids) * 0.15))))  # 15 % of tokens in one sentence\n","            cand_masked_pos = [i for i, token in enumerate(input_ids)\n","                               if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n","            shuffle(cand_masked_pos)\n","\n","            # remove N and gaps from cand_masked_pos\n","            cand_masked_pos_copy = cand_masked_pos.copy()\n","            for position in cand_masked_pos_copy:\n","                key = list(word_dict.keys())[list(word_dict.values()).index(position)]\n","                if (\"N\" in key) or (\"-\" in key):\n","                    cand_masked_pos.remove(position)\n","            # if the position remains is less than 15%, mask them all\n","            if len(cand_masked_pos) < n_pred:\n","                n_pred = len(cand_masked_pos)\n","\n","            masked_tokens, masked_pos = [], []\n","            for pos in cand_masked_pos[:n_pred]:\n","                masked_pos.append(pos)\n","                masked_tokens.append(input_ids[pos])\n","                # if random() < 0.8:  # 80%\n","                input_ids[pos] = word_dict['[MASK]']  # make mask\n","                # elif random() < 0.5:  # 10%\n","                #     index = randint(0, vocab_size - 1)  # random index in vocabulary\n","                #     input_ids[pos] = word_dict[number_dict[index]]  # replace\n","\n","            # Zero Paddings\n","            n_pad = max_len - len(input_ids)\n","            input_ids.extend([0] * n_pad)\n","            segment_ids.extend([0] * n_pad)\n","\n","            # Zero Padding (100% - 15%) tokens\n","            if max_mask_count > n_pred:\n","                n_pad = max_mask_count - n_pred\n","                masked_tokens.extend([0] * n_pad)\n","                masked_pos.extend([0] * n_pad)\n","\n","            if is_positive and positive < data_count / 2:\n","                self.batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])  # IsNext\n","                positive += 1\n","            elif not is_positive and negative < data_count / 2:\n","                self.batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])  # NotNext\n","                negative += 1\n","\n","    def __len__(self):\n","        return len(self.batch)\n","\n","    def __getitem__(self, idx):\n","        ids = torch.Tensor(self.batch[idx][0])\n","        seg = torch.Tensor(self.batch[idx][1])\n","        msk_tok = torch.Tensor(self.batch[idx][2])\n","        msk_pos = torch.Tensor(self.batch[idx][3])\n","        label = torch.Tensor([self.batch[idx][4]])\n","\n","        ids, seg, msk_pos = ids.type(torch.IntTensor), seg.type(torch.IntTensor), msk_pos.type(torch.int64)\n","\n","        msk_tok = msk_tok.type(torch.LongTensor)\n","        label = label.type(torch.LongTensor)\n","\n","        return ids, seg, msk_pos, msk_tok, label\n"],"metadata":{"id":"WuvfJQ4MINGk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"EEHIASapIyGz"}},{"cell_type":"code","source":["class Embedding(nn.Module):\n","    def __init__(self, vocab_size, d_model, maxlen, n_segments):\n","        super(Embedding, self).__init__()\n","        self.tok_embed = nn.Embedding(int(vocab_size), int(d_model))  # token embedding\n","        self.pos_embed = nn.Embedding(int(maxlen), int(d_model))  # position embedding\n","        self.seg_embed = nn.Embedding(int(n_segments), int(d_model))  # segment(token type) embedding\n","        self.norm = nn.LayerNorm(int(d_model))\n","\n","    def forward(self, x, seg):\n","        seq_len = x.size(1)\n","        pos = torch.arange(seq_len, dtype=torch.long)\n","        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n","        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n","        return self.norm(embedding)\n","\n","\n","class GELU(nn.Module):\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","\n","\n","def get_attn_pad_mask(seq_q, seq_k):\n","    batch_size, len_q = seq_q.size()\n","    batch_size, len_k = seq_k.size()\n","    # eq(zero) is PAD token\n","    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n","    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n","\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, d_k, d_v, n_heads):\n","        super(EncoderLayer, self).__init__()\n","        self.enc_self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads)\n","        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_k)\n","\n","    def forward(self, enc_inputs, enc_self_attn_mask):\n","        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs,\n","                                               enc_self_attn_mask)  # enc_inputs to same Q,K,V\n","        enc_outputs = self.pos_ffn(enc_outputs)  # enc_outputs: [batch_size x len_q x d_model]\n","        return enc_outputs, attn\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, d_k, d_v, n_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n","        self.W_K = nn.Linear(d_model, d_k * n_heads)\n","        self.W_V = nn.Linear(d_model, d_v * n_heads)\n","\n","        self.d_k = d_k\n","        self.d_v = d_v\n","        self.n_heads = n_heads\n","        self.d_model = d_model\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n","        residual, batch_size = Q, Q.size(0)\n","        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n","        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,\n","                                                                                 2)  # q_s: [batch_size x n_heads x len_q x d_k]\n","        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,\n","                                                                                 2)  # k_s: [batch_size x n_heads x len_k x d_k]\n","        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,\n","                                                                                 2)  # v_s: [batch_size x n_heads x len_k x d_v]\n","\n","        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1,\n","                                                  1)  # attn_mask : [batch_size x n_heads x len_q x len_k]\n","\n","        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n","        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s, attn_mask)\n","        context = context.transpose(1, 2).contiguous().view(batch_size, -1,\n","                                                            self.n_heads * self.d_v)  # context: [batch_size x len_q x n_heads * d_v]\n","        output = nn.Linear(self.n_heads * self.d_v, self.d_model)(context)\n","\n","        return nn.LayerNorm(self.d_model)(output + residual), attn  # output: [batch_size x len_q x d_model]\n","\n","\n","class PoswiseFeedForwardNet(nn.Module):\n","\n","    def __init__(self, d_model, d_ff):\n","        super(PoswiseFeedForwardNet, self).__init__()\n","        self.l1 = nn.Linear(d_model, d_ff)\n","        self.l2 = nn.Linear(d_ff, d_model)\n","\n","        self.relu = GELU()\n","        self.layer_norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, inputs):\n","        residual = inputs\n","        output = self.l1(inputs)\n","        output = self.relu(output)\n","        output = self.l2(output)\n","        return self.layer_norm(output + residual)\n","\n","\n","class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, d_k):\n","        self.d_k = d_k\n","        super(ScaledDotProductAttention, self).__init__()\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(\n","            self.d_k)  # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n","        scores.masked_fill_(attn_mask, -1e9)  # Fills elements of self tensor with value where mask is one.\n","        attn = nn.Softmax(dim=-1)(scores)\n","        context = torch.matmul(attn, V)\n","        return context, attn\n","\n","\n","class BERT(nn.Module):\n","    def __init__(self, vocab_size, d_model, maxlen, n_segments, n_layers, d_k, d_v, n_heads):\n","        super(BERT, self).__init__()\n","        self.embedding = Embedding(vocab_size, d_model, maxlen, n_segments)\n","        self.layers = nn.ModuleList([EncoderLayer(d_model, d_k, d_v, n_heads) for _ in range(n_layers)])\n","        self.fc = nn.Linear(d_model, d_model)\n","        self.activ1 = nn.Tanh()\n","        self.linear = nn.Linear(d_model, d_model)\n","        self.activ2 = GELU()\n","        self.norm = nn.LayerNorm(d_model)\n","        self.classifier = nn.Linear(d_model, 2)\n","        # decoder is shared with embedding layer\n","        embed_weight = self.embedding.tok_embed.weight\n","        n_vocab, n_dim = embed_weight.size()\n","        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n","        self.decoder.weight = embed_weight\n","        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n","\n","    def forward(self, input_ids, segment_ids, masked_pos):\n","        output = self.embedding(input_ids, segment_ids)\n","        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n","        for layer in self.layers:\n","            # embedding layer\n","            output, enc_self_attn = layer(output, enc_self_attn_mask)\n","\n","        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n","        # classifier pos/neg (it will be decided by first token(CLS))\n","        h_pooled = self.activ1(self.fc(output[:, 0]))  # [batch_size, d_model]\n","        logits_clsf = self.classifier(h_pooled)  # [batch_size, 2]\n","\n","        # classifier mask\n","        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))  # [batch_size, max_pred, d_model]\n","        # get masked position from final output of transformer.\n","        h_masked = torch.gather(output, 1, masked_pos)  # masking position [batch_size, max_pred, d_model]\n","        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n","        logits_lm = self.decoder(h_masked) + self.decoder_bias  # [batch_size, max_pred, n_vocab]\n","\n","        return logits_lm, logits_clsf, output"],"metadata":{"id":"jnNFTO3xIzcO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train\n"],"metadata":{"id":"WCsAwf11JJDi"}},{"cell_type":"markdown","source":["Mount your drive to access the data"],"metadata":{"id":"OkKDUYSeWapx"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKfdWluKJrMs","executionInfo":{"status":"ok","timestamp":1678383845655,"user_tz":300,"elapsed":25210,"user":{"displayName":"Monireh Safari","userId":"04410739624450238233"}},"outputId":"7d5e2169-11d7-4ba8-c0b9-c31aad9bf8cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# File path of the input\n","# input_path = \"/content/drive/MyDrive/BIOSCAN/full_training.tsv\"\n","# input_path = \"/content/drive/MyDrive/BIOSCAN/medium_training.tsv\"\n","input_path = \"/content/drive/MyDrive/BIOSCAN/small_training.tsv\"\n","\n","\n","# dataloader to get a batch of data\n","dataset = SampleDNAData(file_path=input_path, k_mer=4, data_count=256, max_mask_count=70, max_len=512)\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n","\n","# define the model\n","# DNABert configuration\n","config = {\n","    \"d_model\": 768,\n","    \"n_heads\": 12,\n","    \"n_layers\": 12 \n","\n","}\n","# # first config\n","# config = {\n","#     \"d_model\": 16,\n","#     \"n_heads\": 2,\n","#     \"n_layers\": 5 \n","\n","# }\n","model = BERT(dataset.vocab_size, config[\"d_model\"], dataset.max_len, 2, config[\"n_layers\"], 32, 32, config[\"n_heads\"])\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"hEshydwlBusq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["saving_path = \"/content/drive/MyDrive/BIOSCAN/model_checkpoints_new/\"\n","epoch_loss_list = []\n","training_epoch = 500\n","continue_epoch = 0\n","\n","''' \n","If you lost your connection and want to continue the training process, \n","uncomment this part, load your last model, optimizer, and loss, \n","choose the epoch you \n","want to continue from\n","'''\n","# continue_epoch = 100\n","# model.load_state_dict(torch.load(saving_path + f'model_{continue_epoch}.pth'))\n","# optimizer.load_state_dict(torch.load(saving_path + f\"optimizer_{continue_epoch}.pth\"))\n","\n","# a_file = open(saving_path + \"loss.pkl\", \"rb\")\n","# epoch_loss_list = pickle.load(a_file)\n","\n"],"metadata":{"id":"UR3ajJsf4ZN3","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1678383854741,"user_tz":300,"elapsed":7,"user":{"displayName":"Monireh Safari","userId":"04410739624450238233"}},"outputId":"7caf645f-5fe2-477c-fb2b-d3c4ad424237"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' \\nIf you lost your connection and want to continue the training process, \\nuncomment this part, load your last model, optimizer, and loss, \\nchoose the epoch you \\nwant to continue from\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# strat training\n","for epoch in range(continue_epoch, training_epoch):\n","    epoch_loss = 0\n","    for ids, seg, msk_pos, masked_tokens, is_pos in dataloader:\n","        optimizer.zero_grad()\n","        logits_lm, logits_clsf, outputs= model(ids, seg, msk_pos)\n","\n","        loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)  # for masked LM\n","        loss_lm = (loss_lm.float()).mean()\n","        loss_clsf = criterion(logits_clsf, torch.squeeze(is_pos))  # for sentence classification\n","        loss = loss_lm + loss_clsf\n","\n","        epoch_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","    \n","    epoch_loss_list.append(epoch_loss)\n","\n","    print(f\"epoch {epoch}: Loss is {epoch_loss}\")\n","\n","    # every 50 epoch save the checkpoints and save the loss in a list\n","    if epoch % 50 == 0:\n","        torch.save(model.state_dict(), saving_path + \"model_\" + str(epoch) +'.pth')\n","        torch.save(optimizer.state_dict(), saving_path + \"optimizer_\" + str(epoch) +'.pth')\n","\n","        a_file = open(saving_path + \"loss.pkl\", \"wb\")\n","        pickle.dump(epoch_loss_list, a_file)\n","        a_file.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"MLAbcratJLHT","executionInfo":{"status":"error","timestamp":1678416969714,"user_tz":300,"elapsed":830819,"user":{"displayName":"Monireh Safari","userId":"04410739624450238233"}},"outputId":"64e2a4f1-cf63-4198-897b-ce7a70c35077"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0: Loss is 3077.4000148773193\n","epoch 1: Loss is 1284.2528228759766\n","epoch 2: Loss is 577.8037328720093\n","epoch 3: Loss is 447.2536668777466\n","epoch 4: Loss is 371.2786350250244\n","epoch 5: Loss is 348.15738582611084\n","epoch 6: Loss is 342.93158745765686\n","epoch 7: Loss is 420.5345735549927\n","epoch 8: Loss is 396.643342256546\n","epoch 9: Loss is 372.49149990081787\n","epoch 10: Loss is 533.1799840927124\n","epoch 11: Loss is 482.9966721534729\n","epoch 12: Loss is 385.9531583786011\n","epoch 13: Loss is 352.0544571876526\n","epoch 14: Loss is 336.9014103412628\n","epoch 15: Loss is 353.75666427612305\n","epoch 16: Loss is 482.92566204071045\n","epoch 17: Loss is 474.91187143325806\n","epoch 18: Loss is 370.27308106422424\n","epoch 19: Loss is 344.2973601818085\n","epoch 20: Loss is 387.16072034835815\n","epoch 21: Loss is 349.3293697834015\n","epoch 22: Loss is 328.1490921974182\n","epoch 23: Loss is 364.0607998371124\n","epoch 24: Loss is 388.6774709224701\n","epoch 25: Loss is 356.7534091472626\n","epoch 26: Loss is 341.5543751716614\n","epoch 27: Loss is 333.6438570022583\n","epoch 28: Loss is 351.3304195404053\n","epoch 29: Loss is 382.0618510246277\n","epoch 30: Loss is 367.03964734077454\n","epoch 31: Loss is 385.3014817237854\n","epoch 32: Loss is 371.47067189216614\n","epoch 33: Loss is 371.1297836303711\n","epoch 34: Loss is 384.5610213279724\n","epoch 35: Loss is 361.92726707458496\n","epoch 36: Loss is 359.7814426422119\n","epoch 37: Loss is 375.62350273132324\n","epoch 38: Loss is 359.77979040145874\n","epoch 39: Loss is 362.3610186576843\n","epoch 40: Loss is 359.1931359767914\n","epoch 41: Loss is 357.222998380661\n","epoch 42: Loss is 358.55532789230347\n","epoch 43: Loss is 367.1669101715088\n","epoch 44: Loss is 355.3925838470459\n","epoch 45: Loss is 357.6242413520813\n","epoch 46: Loss is 360.7441864013672\n","epoch 47: Loss is 363.82720947265625\n","epoch 48: Loss is 353.57570123672485\n","epoch 49: Loss is 369.93067741394043\n","epoch 50: Loss is 355.61259841918945\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-6049bc861c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# every 50 epoch save the checkpoints and save the loss in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"model_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"optimizer_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Parent directory /content/drive/MyDrive/BIOSCAN/model_checkpoints does not exist."]}]},{"cell_type":"markdown","source":["## Plot the loss after training"],"metadata":{"id":"TmfJg62-Ahp1"}},{"cell_type":"code","source":["plt.plot(epoch_loss_list, label =\"Epoch Loss\")\n","plt.legend(loc='upper right')\n","plt.savefig('/content/drive/MyDrive/BIOSCAN/model_checkpoints_new/loss_per_epoch.png')\n","plt.show()"],"metadata":{"id":"K4v-JAdIWzXg","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1678417401596,"user_tz":300,"elapsed":360,"user":{"displayName":"Monireh Safari","userId":"04410739624450238233"}},"outputId":"778a1b6a-6303-4b14-dfa7-cf7d01efca46"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmiUlEQVR4nO3deZxU1Z338c+vqrqqu3qFpmmgG2wQXFgEFXFHo4kSMxPjE6MhiSITZZIYo7Mko5N5Hk1GExOTmGUyGh1RnHEjUaOjRkPc0AQNiwjI2iJLNw3dQEPT9F51nj/qVtMIbe8U1v2+X6961a1T91ad0xTfOnXuufeacw4REfGHQKorICIiR45CX0TERxT6IiI+otAXEfERhb6IiI+EUl2BjzJkyBBXVlaW6mqIiHysLF26dKdzruhwzx3VoV9WVsaSJUtSXQ0RkY8VM9vc2XMa3hER8RGFvoiIjyj0RUR85Kge0xeRj6fW1lYqKipoampKdVXSWmZmJqWlpWRkZHR7G4W+iPS7iooKcnNzKSsrw8xSXZ205Jxj165dVFRUMHr06G5vp+EdEel3TU1NFBYWKvAHkJlRWFjY419TCn0RGRAK/IHXm79xWob+vqZW7l6wnuVb96S6KiIiR5W0DP22mOMXL2/gnS21qa6KiKRIMBhkypQp7bc777yz315706ZNTJw4scv1brvtNn7yk5/02/v2h7TckRuNBAFoaImluCYikipZWVksX7481dU46qRlTz8cDBAKGPub21JdFRE5ypSVlfGd73yHSZMmMW3aNMrLy4FE7/2CCy7gpJNO4sILL2TLli0A7Nixg8suu4zJkyczefJk/vKXvwAQi8W47rrrmDBhAhdddBGNjY3den/nHN/+9reZOHEikyZN4oknngCgqqqK6dOnM2XKFCZOnMgbb7xBLBbjmmuuaV/37rvv7nP7u+zpm1kmsBCIeOv/zjl3q5mNBh4HCoGlwFXOuRYziwAPA6cCu4ArnXObvNe6BfgqEAO+5Zx7qc8tOHydiYaD6umLHAW+97/vsXpbXb++5vgRedz6txM+cp3GxkamTJnS/viWW27hyiuvBCA/P5+VK1fy8MMPc9NNN/Hcc89xww03MGvWLGbNmsXcuXP51re+xe9//3u+9a1vcd555/H0008Ti8Wor6+ntraWDRs28Nhjj3H//fdzxRVX8OSTT/KVr3yly7o/9dRTLF++nHfffZedO3dy2mmnMX36dB599FEuvvhivvvd7xKLxWhoaGD58uVUVlayatUqAPbs2dPrv1lSd3r6zcAFzrnJwBRghpmdAfwIuNs5NxaoJRHmePe1Xvnd3nqY2Xjgi8AEYAbwn2YW7HMLOpETCamnL+JjyeGd5C0Z+AAzZ85sv1+0aBEAixYt4ktf+hIAV111FW+++SYAr7zyCl//+teBxH6C/Px8AEaPHt3+pXLqqaeyadOmbtXrzTffZObMmQSDQYqLiznvvPNYvHgxp512Gg8++CC33XYbK1euJDc3lzFjxrBx40ZuuOEGXnzxRfLy8vr8d+myp+8SV06v9x5meDcHXAB8ySufB9wG3ANc6i0D/A74D0vMK7oUeNw51wx8YGblwDRgUZ9bcRjRSIj9LQp9kVTrqkeeCh2nOvZ2amkkEmlfDgaD3R7e6cz06dNZuHAhzz//PNdccw3/+I//yNVXX827777LSy+9xL333sv8+fOZO3dun96nW2P6ZhY0s+VANbAAeB/Y45xLpmoFUOItlwBbAbzn95IYAmovP8w2Hd9rjpktMbMlNTU1PW5QUnY4yP5mDe+IyKGS4+hPPPEEZ555JgBnnXUWjz/+OACPPPII5557LgAXXngh99xzD5AYx9+7d2+f3vvcc8/liSeeIBaLUVNTw8KFC5k2bRqbN2+muLiY6667jmuvvZZly5axc+dO4vE4n//857n99ttZtmxZn94bujl7xzkXA6aYWQHwNHBCn9+58/e6D7gPYOrUqa63rxMNh2hQT1/Etz48pj9jxoz2aZu1tbWcdNJJRCIRHnvsMQB+9atfMXv2bO666y6Kiop48MEHAfjFL37BnDlzeOCBBwgGg9xzzz0MHz682/W4/fbb+fnPf97+eOvWrSxatIjJkydjZvz4xz9m2LBhzJs3j7vuuouMjAxycnJ4+OGHqaysZPbs2cTjcQB++MMf9vGvApYYvenBBmb/D2gE/gUY5pxrM7Mzgduccxeb2Uve8iIzCwHbgSLgZgDn3A+912lfr7P3mjp1quvtRVSunbeYbXuaeOHGc3u1vYj03po1azjxxBNTXY3DSl6caciQIamuSr843N/azJY656Yebv0uh3fMrMjr4WNmWcCngDXAq8Dl3mqzgGe85We9x3jPv+LtF3gW+KKZRbyZP+OAv3a/aT2jnr6IyKG6M7wzHJjnzbQJAPOdc8+Z2WrgcTO7HXgHeMBb/wHgv70dtbtJzNjBOfeemc0HVgNtwPXesNGAyI4E2a8pmyLyId2dZZOuujN7ZwVw8mHKN5KYffPh8ibgC5281h3AHT2vZs9FwyEaNGVTJGWcczrp2gDr6fA8pOkRuZCYvdPQGiMe7/W+YBHppczMTHbt2tWrUJLuSZ5PPzMzs0fbpeW5dyAxT985aGqLEQ2nbTNFjkqlpaVUVFTQl2nX0rXklbN6Im3TMDucONh3f7NCX+RIy8jI6NHVnOTISd/hnUgi6DWDR0TkgLQN/WTvXkfliogckLahn+2dU1/n3xEROSBtQ/9AT1+hLyKSlLahn62rZ4mIHCJ9Q189fRGRQ6Rt6EfD6umLiHxY2oZ+csqmduSKiByQtqEfCQUIGDRoyqaISLu0DX0zIzusSyaKiHSUtqEPEI0E1dMXEekgrUNfPX0RkYOld+hHQpq9IyLSQVqHfjQcpF7z9EVE2qV16Cd6+gp9EZGktA79aFg7ckVEOkrr0NeOXBGRg6V16GvKpojIwdI69JM9fV2cWUQkIa1DPxoJEnfQ3BZPdVVERI4KaR36Or2yiMjB0jr0dXplEZGDpXXo6/TKIiIH80foawaPiAjQjdA3s5Fm9qqZrTaz98zsRq/8NjOrNLPl3u2SDtvcYmblZrbOzC7uUD7DKys3s5sHpkkHZLcP76inLyICEOrGOm3APznnlplZLrDUzBZ4z93tnPtJx5XNbDzwRWACMAL4k5kd5z39a+BTQAWw2Myedc6t7o+GHE5UO3JFRA7SZeg756qAKm95n5mtAUo+YpNLgcedc83AB2ZWDkzznit3zm0EMLPHvXUHLPSzI4mevoZ3REQSejSmb2ZlwMnA217RN81shZnNNbNBXlkJsLXDZhVeWWflH36POWa2xMyW1NTU9KR6h0j29DW8IyKS0O3QN7Mc4EngJudcHXAPcCwwhcQvgZ/2R4Wcc/c556Y656YWFRX16bXae/qasikiAnRvTB8zyyAR+I84554CcM7t6PD8/cBz3sNKYGSHzUu9Mj6ifEBkhoKYQYPG9EVEgO7N3jHgAWCNc+5nHcqHd1jtMmCVt/ws8EUzi5jZaGAc8FdgMTDOzEabWZjEzt5n+6cZhxcIGNGMoHr6IiKe7vT0zwauAlaa2XKv7F+BmWY2BXDAJuDvAZxz75nZfBI7aNuA651zMQAz+ybwEhAE5jrn3uu3lnQiqgupiIi0687snTcBO8xTL3zENncAdxym/IWP2m4gZIeDmr0jIuJJ6yNyITGDRz19EZGEtA/9nEhIPX0REU/ah340ElRPX0TEk/ahnx0OUa8pmyIigA9CPxoO6nz6IiKetA/97EhIJ1wTEfGkfegne/q6OLqIiA9CPzsSoi3uaInp4ugiImkf+u3XydW0TRGR9A/97LCukysikpT2oR+NJC+ZqJ6+iEjah362LpkoItIu/UM/krx6lnr6IiJpH/rJHbnq6YuI+CD01dMXETkg/UPf6+nr/DsiIj4I/Wh7T1+hLyKS9qGflZEc09fwjohI2od+MGBkZeic+iIi4IPQB8iOBNmvHbkiIv4I/Wg4RIN25IqI+CX01dMXEQGfhH52JKQxfRERfBL60XBQs3dERPBJ6Oeopy8iAvgk9KPhkHr6IiL4JPQTUzbV0xcR6TL0zWykmb1qZqvN7D0zu9ErH2xmC8xsg3c/yCs3M/ulmZWb2QozO6XDa83y1t9gZrMGrlkHS0zZVE9fRKQ7Pf024J+cc+OBM4DrzWw8cDPwsnNuHPCy9xjg08A47zYHuAcSXxLArcDpwDTg1uQXxUDLDgdpicVpadPF0UXE37oMfedclXNumbe8D1gDlACXAvO81eYBn/OWLwUedglvAQVmNhy4GFjgnNvtnKsFFgAz+rMxnUmedK1Rc/VFxOd6NKZvZmXAycDbQLFzrsp7ajtQ7C2XAFs7bFbhlXVW/uH3mGNmS8xsSU1NTU+q16nk6ZU1ri8iftft0DezHOBJ4CbnXF3H55xzDnD9USHn3H3OuanOualFRUX98ZI6vbKIiKdboW9mGSQC/xHn3FNe8Q5v2AbvvtorrwRGdti81CvrrHzAtff0tTNXRHyuO7N3DHgAWOOc+1mHp54FkjNwZgHPdCi/2pvFcwaw1xsGegm4yMwGeTtwL/LKBlw0nOjpa3hHRPwu1I11zgauAlaa2XKv7F+BO4H5ZvZVYDNwhffcC8AlQDnQAMwGcM7tNrN/BxZ7633fObe7PxrRlexIoqevaZsi4nddhr5z7k3AOnn6wsOs74DrO3mtucDcnlSwPyQvjq6evoj4nT+OyA0nd+Sqpy8i/uaL0I9Gkjty1dMXEX/zR+jr4ugiIoBPQj8UDBAJBTRPX0R8zxehD4mdudqRKyJ+55vQj4aDmrIpIr7nm9DPDqunLyLim9CPRoKasikivueb0M8OhzRlU0R8zzehHw2rpy8i4pvQz9HsHRER/4R+NKLZOyIivgl9zd4REfFR6EfDIZpa48Ti/XKBLxGRjyXfhH7ynPrq7YuIn/km9JNXz9K4voj4mW9CXz19EREfhb56+iIiPgr97LB6+iIivgn9aCR5yUSFvoj4l29Cv72nr+EdEfEx/4S+evoiIj4KfW9Hrnr6IuJnvgn9LG94Rz19EfEz34R+OBQgHAywX6dXFhEf803oQ+JMm7qQioj4WZehb2ZzzazazFZ1KLvNzCrNbLl3u6TDc7eYWbmZrTOzizuUz/DKys3s5v5vStcSV89ST19E/Ks7Pf2HgBmHKb/bOTfFu70AYGbjgS8CE7xt/tPMgmYWBH4NfBoYD8z01j2iElfPUk9fRPwr1NUKzrmFZlbWzde7FHjcOdcMfGBm5cA077ly59xGADN73Ft3dc+r3HvRSEhj+iLia30Z0/+mma3whn8GeWUlwNYO61R4ZZ2VH1HZ4SANGtMXER/rbejfAxwLTAGqgJ/2V4XMbI6ZLTGzJTU1Nf31skDipGvq6YuIn/Uq9J1zO5xzMedcHLifA0M4lcDIDquWemWdlR/ute9zzk11zk0tKirqTfU6lR3RmL6I+FuvQt/Mhnd4eBmQnNnzLPBFM4uY2WhgHPBXYDEwzsxGm1mYxM7eZ3tf7d7Jjmj2joj4W5c7cs3sMeB8YIiZVQC3Aueb2RTAAZuAvwdwzr1nZvNJ7KBtA653zsW81/km8BIQBOY6597r78Z0JVuzd0TE57oze2fmYYof+Ij17wDuOEz5C8ALPapdP4uGQzS0xIjHHYGApbIqIiIp4asjcpOXTGxs1RCPiPiTr0I/eclEXT1LRPzKV6HffnF07cwVEZ/yVei39/R1gJaI+JSvQj95IZUGHaAlIj7lq9CPJod3NKYvIj7lq9Bv7+lrTF9EfMpXoR8Nq6cvIv7mq9DPjiR7+gp9EfEnn4V+sqev4R0R8SdfhX44GCAzI0Dt/pZUV0VEJCV8FfpmxoiCLLbtbUx1VUREUsJXoQ9QUpBFZa1CX0T8yZ+hv0ehLyL+5MvQ31nfQpPOtCkiPuS/0B+UBcA29fZFxId8F/ojChKhryEeEfEj34V+STL0tTNXRHzId6E/LD+TgGl4R0T8yXehnxEMUJyXSYVCX0R8yHehD5qrLyL+5c/QH6SjckXEn3wZ+iMKsqja00Qs7lJdFRGRI8qXoV9SkEVb3FG9rynVVREROaL8Gfo6QEtEfMqfoe/N1a/QzlwR8Rlfh76OyhURv+ky9M1srplVm9mqDmWDzWyBmW3w7gd55WZmvzSzcjNbYWandNhmlrf+BjObNTDN6Z7sSIiCaIaGd0TEd7rT038ImPGhspuBl51z44CXvccAnwbGebc5wD2Q+JIAbgVOB6YBtya/KFJlRL7m6ouI/3QZ+s65hcDuDxVfCszzlucBn+tQ/rBLeAsoMLPhwMXAAufcbudcLbCAQ79IjqiSQTqvvoj4T2/H9Iudc1Xe8nag2FsuAbZ2WK/CK+usPGWSR+U6p7n6IuIffd6R6xKp2W/JaWZzzGyJmS2pqanpr5c9ROmgLPa3xKhrbBuw9xAROdr0NvR3eMM2ePfVXnklMLLDeqVeWWflh3DO3eecm+qcm1pUVNTL6nUteV79ij0NA/YeIiJHm96G/rNAcgbOLOCZDuVXe7N4zgD2esNALwEXmdkgbwfuRV5ZyiSnbW7bo6NyRcQ/Ql2tYGaPAecDQ8ysgsQsnDuB+Wb2VWAzcIW3+gvAJUA50ADMBnDO7TazfwcWe+t93zn34Z3DR1TyqNzKWvX0RcQ/ugx959zMTp668DDrOuD6Tl5nLjC3R7UbQIXZYSKhgGbwiIiv+PKIXAAzo6QgS8M7IuIrvg19SAzx6ApaIuInvg59HZUrIn7j69AvGZTFzvpmmlpjqa6KiMgR4e/Q96ZtVu3VuL6I+IOvQz95gJaGeETEL3wd+qW6gpaI+IyvQ39YfiZmaAaPiPiGr0M/IxigODdTwzsi4hu+Dn1IzODR8I6I+IVCv0AXUxER//B96I8oyKJqbyPxuC6mIiLpz/ehXzIoi9aYo6a+OdVVEREZcL4P/dLkxVS0M1dEfMD3od9+gJbG9UXEB3wf+gcupqLQF5H05/vQz4mEyM/K0LRNEfEF34c+JIZ4NLwjIn6g0Mebq6/hHRHxAYU+iROvaXhHRPxAoQ+MKMhkX3MbextbU10VEZEBpdAHSgqigGbwiEj6U+hzYNqmhnhEJN0p9EkM74AO0BKR9KfQB4ZkRwiHAnywc3+qqyIiMqAU+kAgYJx3XBFPLN7K1t0Nqa6OiMiAUeh7vvfZCQQMbnlqJc7pNMsikp76FPpmtsnMVprZcjNb4pUNNrMFZrbBux/klZuZ/dLMys1shZmd0h8N6C8jCrK4+ZITebN8J79dWpHq6vRYY0uMX79azvs19amuiogcxfqjp/8J59wU59xU7/HNwMvOuXHAy95jgE8D47zbHOCefnjvfvXlaaOYVjaY259bTXVdU6qr0yMPvLmRu15ax4yfL+SHf1hDfXNbqqskIkehgRjeuRSY5y3PAz7Xofxhl/AWUGBmwwfg/XstEDDu/Pwkmtri/L9n3kt1dbptT0MLv1m4kXPHDeFzU0r4zesbueAnr/HM8koNVYnIQfoa+g74o5ktNbM5Xlmxc67KW94OFHvLJcDWDttWeGVHlTFFOfzDJ4/jxfe284eVVV1vcBS45/X3qW9u498+M567vjCZp75xFsV5mdz4+HKu/M1brKmqS3UVReQoEerj9uc45yrNbCiwwMzWdnzSOefMrEddTe/LYw7AqFGj+li93rnu3NE8v3Ib//eZ9zjz2EIKouGU1KM7dtQ18dCfN/G5KSUcPywXgFNGDeL315/N/CVb+fGLa/nML9/gx5dP5vJTS49o3f716ZX8YWUVQ3IiDM2LUJQToSg3cZtaNphTRg06ovURkT729J1zld59NfA0MA3YkRy28e6rvdUrgZEdNi/1yj78mvc556Y656YWFRX1pXq9FgoG+NHnT6K2oYXbn1+Tkjp01y9f3kAs7viHTx53UHkwYMycNopX//l8zjy2kJufXMEbG2qOWL0WrN7Bo29vYWJJPscW5dDUGmfplloeXrSZH7ywlivuXcRbG3cdsfqISEKvQ9/Mss0sN7kMXASsAp4FZnmrzQKe8ZafBa72ZvGcAeztMAx01JkwIp+vnTeG3y2t4LV11V1vkAKbd+3nicVbmTltFKMKo4ddpyAa5t6vnMrYoTl8/X+WHZGhnr2NrXz36ZWcMCyXB2adxr1XncqTXz+LN75zAWv/fQaLv/tJRhVG+cYjy6io1XERIkdSX3r6xcCbZvYu8Ffgeefci8CdwKfMbAPwSe8xwAvARqAcuB/4Rh/e+4i44YJxjB2aw/WPLDuiveTu+tmC9YSCxg0XjP3I9XIzM3hw9mlkR4LMfnAx2/cO7MykO55fza79Ldx1+WTCoYM/YmZGUW6E+6+eSmtbnDkPL6WxJTag9RGRA3od+s65jc65yd5tgnPuDq98l3PuQufcOOfcJ51zu71y55y73jl3rHNuknNuSX81YqBkZgR55NrTGTk4yuwHF/P7dw4ZjeqzbXsaeWLxFjbs2NejmTZrqup49t1tzD57NEPzMrtcf3h+Fg9eM4365jZmP7SYfU0DcxrpNzbUMH9JBXOmj2FSaX6n6x1blMMvZ57Mmu11fPt372qWkcgRoiNyu1Ccl8n8r53J1LJB3PTEcu5b+H6/BdSaqjou/fWf+ZcnV/Kpuxdy9p2vcMtTK3hx1fYuQ/mnf1xHTiTE16Yf2+33Gz8ij19/+RTW79jH9Y++Q2ss3tcmHKS+uY2bn1zJmKJsbrxwXJfrf+KEofzzRcfz3Ioq7n19Y7/WRUQOT6HfDXmZGcz7u2l8ZtJwfvDCWm5/fg3xeN+C/68f7OaK3ywiFDAeve50fnDZJCaV5vO/71bxtf9ZysnfX8AVv1nEf75WzuptdQd90SzdvJs/ranma+cdS340o0fve95xRfzgsoksXF/Dvz29ql972D9+cS3b9jZy1+UnkZkR7NY23zj/WP7mpOH8+KW1vNqLfScD/Quham8jTa0afpL0YUfzz+qpU6e6JUuOnlGgeNzx/edW89BfNvG3k0fwky+cRCTUvXDr6E+rd3D9o8soGZTFf3/1dEoKstqfa43FWbq5ltfX1/Daupr2Ha/FeRHOO66I848fykN/2cTGmv0s/M75RMO9m3X70z+u41evlHPmmEK+csYxfGp88SHj7z3x9sZdXHnfW8w+u4xb/3ZCj7ZtaGnj8/csoqK2gWeuP5sxRTldbrOiYg93L1jPwg07mTFhGF89d3S/TAFtbovx9sbdvLqumtfW1fDBzv3kZ2Xw2ckjuPzUUk4qzcfM+vw+H+WDnft5elkFG3fuZ+zQHE4YlscJw3IZNThKIJB4b+ccFbWNvLN1D8u37GH51lq27G5k1OAsxg3NZVxxDmOH5jCuOJcR+ZmHrbNzjp31LbxfU095dT3v19Tzfs1+tu5uYOzQHM4dN4Szxw5hzJDsHre5pS1OeXU9mRkBygqz2+stR4aZLe1wloSDn1Po94xzjntf38iPXlxLWWGUT5wwlOnHFXH66MHdCuDfLa3gX55cwcQReTw4exqDsz/6GIAddU28vq6G19ZX88aGnexrSpxe4XufncCss8r61I7/euMDHvrLJir3NFKYHebyqaXMPG0UZUOye/RajS0xPv2LhcSc46Wbpvfqi2jr7gYu/fWfiYQCXH5qKZ88sZhJJfmHhMWqyr38/E/r+dOaagqiGVx4QjELVm+nrqmNU48ZxLXnjOaiCcMI9iBk9je38dyKbSxYXc1f3t9JQ0uMSCjAmccWcvaxQ1i1bS8vrtpOc1uc44pzuPzUUj53cglDc7vel9Jdu/e38NyKbTy1rJLlW/cQsMR+mG17G0n+F42Gg4wrzmVQNIOVFXvZtb8FgMyMAJNK8ikrzGbL7gbKq+vbnwMIhwJkBAwzwwAMDIjFHfs77ESPhoMcW5RDSUEW71XtZevuxPUlhudncvbYIZwzdgjD8jPJCAaIhAJkBAOEQwFCAaOitpHVVXWs3lbH6qo6yqv30Rpz7a97wrBcxo/IY/zwfE4cnsuQnAjNbXGa22I0t8VpaYvT3BbHOUdWRpDMjCBZ4SCZoSCZ4QCRUJBwMEBG0Ah6belMPO5obI1Rva+ZHXVN7Khrorqume11TdQ1tjKiIIuyIVGOKczmmMFRBmeHP/L1nHO0xOI0tcZpao15tziN7csHyppaYxREMygpiFIyKItB0YyDXrulLc76HftYva2OVdv2snpbHQEzxhXncPywXMYNzeW44hwKcyI9/AQdTKE/AF5cVcUjb2/hrx/sprktTjgYYGrZIM4dV8Tk0nzyoxnkZSZuOZkhggHj/oUbueOFNZwzdgj3XnUqOZGehWNbLM6yLXsor67nC1NLyQj2fXQuFne8saGGR9/ewstrq4nFHWePLeQTxw/lxOF5HD8s8R/0w5rbYqzfXs/Kyr38cfV2XltXw6PXns5ZY4f0ui7LttRy5x/WsmTTbuIu8evmwhOL+dT4Ygqzw/zHK+X8cfUO8jJDzJk+hllnlZGbmcH+5jZ+u2Qrc/+8iS27Gxg5OItrzhrNReOLKR2U1el/6A927ue/F23mt0u3sq+pjZKCLC44YSifOKGIM8cMISt84Ffc3sZWnl9RxW+XbuWdLXsIBoyJI/ISvfDhue298UFdfIkntcXirN9Rz4qKPfxpTTWvraumLe44YVgu/+eUEi6dUkJxXiYNLW1s2FHP2u11rN2+j7VV+6htaGHCiHymjCrg5JEFHD8s95DPwq76Zsqr6ymvqWfLrgba4g7nwOHav0TMYOSgKGOH5nDs0ByG52Ue9CW7ZVcDb5TX8Ofynfy5fFe3riE9NDfC+BF5nDg8cWtujR30ZZDstPSFGYkvnGCAgEHcQVs8Tizu2tt5OJkZAXIzM9hZ33zQOrmRUPvV85JfPi2xxBdRS1ucprZYp6/ZlayMICWDshhRkMWu+mbW7zjwZZgdDjJ+RB5xB+t37DvobzMkJ8z0cUX87MopvXpfhf4AamqNsXjTbt7YsJOF62tYu33fYdfLiYSob27jM5OG87MrJ/dqWGig7ahrYv7ircxfurW9lwcwJCfCicNzOb44l/0tbays3Mu67Qc+vHmZIb56zhhu/GTXO2+7o3Z/C6+sreZPa3bw+voaGrzeaG5miGvPGcPsc8rIyzx0X0Ys7liwejv/9cYHLNlcC0BJQRanjxnMGWMKOXNMISUFWby+voZ5izbx2roaMoLGJZOGc/WZx3DKqEHdGsYor67n6XcqWL51D2uq9rG7Q696WF4mxxRG2488Lso9cCTy3sZWVlTs5d2te1i1bS9NrYkd6cV5ES6dUsJlJ5dw4vC8/vgT9rtY3LF2ex17G1rbA7E15mj1loflZ3Li8DyKcjvvoSaHpFZX1bG3sZXMjCCRUOLXQiQUaP8/0dwao9G7JXvUza0xWmOOtlg88Z7ee8fijmDACAWsw32ASEaAobkRivMyKc6LMDQvk9xICDOjqTVGRW0jm3ftZ/OuBjbv2k/lniaCAdp/vURCAe+XRSDxiyMjeQu0/xLJzAgcKA8lfpmEQwFq97dQUdtI5Z5GKmsbqdzTwLY9TRREMxg/Io+JI/KZMCLvoGEv5xw76hJfCsnboGiYWy45sVf/Xgr9I6i6ronymnr2NbVR19hKnXe/r6mN4fmZ/N05o3s09JAqO+ubWbd9H2uqEj3MddsTH8TMjCCTSvKZWJLv3ecxanB0wMa5m1pjLNq4i4rdDXx2ckm3d1yXV+9j0fu7eGvjbt7auKt9uCM7HGR/S4yhuRG+fPoxzJw2sltTXjvjnKOmvpm1VfsSvfGqfVTUNlJT30zNvuZDznYaCQWYWJLP5NICJo/M56TSAsoKB+7vJ/6k0Jd+EY87zPjYBZRzjg3V9by1cRfvVdZx9rghzJgwrE87rruroaWNnftaqN7XRFY4yHHFhw7FiPS3jwr9vp5wTXzk4zoDw8w4rjiX44pzj/h7R8MhRhWGOj1NhsiRpi6HiIiPKPRFRHxEoS8i4iMKfRERH1Hoi4j4iEJfRMRHFPoiIj6i0BcR8ZGj+ohcM6sBNvfhJYYAO/upOh8Xfmuz39oLarNf9KXNxzjnig73xFEd+n1lZks6OxQ5XfmtzX5rL6jNfjFQbdbwjoiIjyj0RUR8JN1D/75UVyAF/NZmv7UX1Ga/GJA2p/WYvoiIHCzde/oiItKBQl9ExEfSMvTNbIaZrTOzcjO7OdX1GQhmNtfMqs1sVYeywWa2wMw2ePeDUlnH/mZmI83sVTNbbWbvmdmNXnnattvMMs3sr2b2rtfm73nlo83sbe8z/oSZde+K7B8TZhY0s3fM7DnvcVq3F8DMNpnZSjNbbmZLvLJ+/2ynXeibWRD4NfBpYDww08zGp7ZWA+IhYMaHym4GXnbOjQNe9h6nkzbgn5xz44EzgOu9f9t0bnczcIFzbjIwBZhhZmcAPwLuds6NBWqBr6auigPiRmBNh8fp3t6kTzjnpnSYn9/vn+20C31gGlDunNvonGsBHgcuTXGd+p1zbiGw+0PFlwLzvOV5wOeOZJ0GmnOuyjm3zFveRyIUSkjjdruEeu9hhndzwAXA77zytGqzmZUCnwH+y3tspHF7u9Dvn+10DP0SYGuHxxVemR8UO+eqvOXtQHEqKzOQzKwMOBl4mzRvtzfUsRyoBhYA7wN7nHNt3irp9hn/OfAdIO49LiS925vkgD+a2VIzm+OV9ftnWxdGT1POOWdmaTkf18xygCeBm5xzdYmOYEI6tts5FwOmmFkB8DRwQmprNHDM7G+AaufcUjM7P8XVOdLOcc5VmtlQYIGZre34ZH99ttOxp18JjOzwuNQr84MdZjYcwLuvTnF9+p2ZZZAI/Eecc095xWnfbgDn3B7gVeBMoMDMkp22dPqMnw181sw2kRiavQD4Benb3nbOuUrvvprEl/s0BuCznY6hvxgY5+3tDwNfBJ5NcZ2OlGeBWd7yLOCZFNal33ljuw8Aa5xzP+vwVNq228yKvB4+ZpYFfIrEvoxXgcu91dKmzc65W5xzpc65MhL/d19xzn2ZNG1vkpllm1luchm4CFjFAHy20/KIXDO7hMS4YBCY65y7I7U16n9m9hhwPonTr+4AbgV+D8wHRpE4JfUVzrkP7+z92DKzc4A3gJUcGO/9VxLj+mnZbjM7icQOvCCJTtp859z3zWwMiZ7wYOAd4CvOuebU1bT/ecM7/+yc+5t0b6/Xvqe9hyHgUefcHWZWSD9/ttMy9EVE5PDScXhHREQ6odAXEfERhb6IiI8o9EVEfEShLyLiIwp9EREfUeiLiPjI/weBSFlbVGxpHAAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]}]}