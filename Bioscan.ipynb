{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN3EsTEzIUcZ"
   },
   "source": [
    "# requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AueLRah0IT-M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randrange, shuffle, random, randint\n",
    "import re\n",
    "\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lURUvE3HuCX"
   },
   "source": [
    "# Load data and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7F8QcTd5HV6a"
   },
   "outputs": [],
   "source": [
    "class PabloDNADataset:\n",
    "    def __init__(self, file_path):\n",
    "        self.df = pd.read_csv(file_path, sep=\"\\t\", encoding=\"unicode_escape\")\n",
    "\n",
    "    def clean_nan(self, col_names, replace_orig=False):\n",
    "        clean_df = self.df.dropna(subset=col_names)\n",
    "        clean_df = clean_df.reset_index(drop=True)\n",
    "        if replace_orig:\n",
    "            self.df = clean_df\n",
    "        return clean_df\n",
    "    \n",
    "    def change_RXY2N(self, col_names, replace_orig=False):\n",
    "      full_pattern = re.compile('[^ACGTN\\-]') \n",
    "      self.df[col_names] = self.df[col_names].apply(lambda x: re.sub(full_pattern, 'N', x))\n",
    "      # if replace_orig:\n",
    "      #   self.df[col_names] = clean_nucleotides\n",
    "      # return clean_str_df\n",
    "\n",
    "\n",
    "    def generate_mini_sample(self, dataframe=None, bin_count=20, output_path=\"mini_sample.tsv\"):\n",
    "        if dataframe is None:\n",
    "            dataframe = self.df\n",
    "        bins = list(dataframe['bin_uri'].unique())\n",
    "        rd1 = random.sample(range(0, len(bins)), bin_count)\n",
    "        bins = [bins[i] for i in rd1]\n",
    "        mini_df = dataframe.loc[dataframe['bin_uri'].isin(bins)]\n",
    "        mini_df = mini_df.reset_index(drop=True)\n",
    "        # mini_df = dataframe.iloc[0:sample_count]\n",
    "        # mini_df = dataframe.take(np.random.permutation(len(dataframe))[:sample_count])\n",
    "        mini_df.to_csv(output_path, sep=\"\\t\")\n",
    "\n",
    "    def get_info(self, dataframe=None):\n",
    "        if dataframe is None:\n",
    "            dataframe = self.df\n",
    "        print(\"Total data: \", len(dataframe))\n",
    "        print(\"Number of bin clusters: \", len(dataframe['bin_uri'].unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WuvfJQ4MINGk"
   },
   "outputs": [],
   "source": [
    "def tokenizer(dna_sentence, k_mer_dict, k_mer_length, stride=1):\n",
    "    tokens = []\n",
    "    for i in range(0, len(dna_sentence) - k_mer_length + 1, stride):\n",
    "        k_mer = dna_sentence[i:i + k_mer_length]\n",
    "        tokens.append(k_mer_dict[k_mer])\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class SampleDNAData(Dataset):\n",
    "    \"\"\"Barcode Dataset\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_all_kmers(k_mer_length, alphabet=None) -> list:\n",
    "        \"\"\"\n",
    "        :rtype: object\n",
    "        \"\"\"\n",
    "        def base_convert(num, base, length):\n",
    "            result = []\n",
    "            while num > 0:\n",
    "                result.insert(0, num % base)\n",
    "                num = num // base\n",
    "            while len(result) < length:\n",
    "                result.insert(0, 0)\n",
    "            return result\n",
    "\n",
    "        if alphabet is None:\n",
    "            alphabet = [\"A\", \"C\", \"G\", \"T\", \"-\", \"N\"]\n",
    "        k_mer_counts = len(alphabet) ** k_mer_length\n",
    "        all_k_mers_list = []\n",
    "        for i in range(k_mer_counts):\n",
    "            code = base_convert(num=i, base=len(alphabet), length=k_mer_length)\n",
    "            k_mer = \"\"\n",
    "            for j in range(k_mer_length):\n",
    "                k_mer += alphabet[code[j]]\n",
    "            all_k_mers_list.append(k_mer)\n",
    "\n",
    "        return all_k_mers_list\n",
    "\n",
    "    \n",
    "    def __init__(self, file_path, k_mer=4, data_count=None, max_mask_count=5, max_len=256):\n",
    "        self.k_mer = k_mer\n",
    "        pablo_dataset = PabloDNADataset(file_path)\n",
    "        if data_count is None:\n",
    "            data_count = len(pablo_dataset.df)\n",
    "        # for removing X,R,Y letters from data\n",
    "        # pablo_dataset.change_RXY2N(\"nucleotides\")\n",
    "        self.dna_nucleotides = list(pablo_dataset.df[\"nucleotides\"].values)\n",
    "        word_list = SampleDNAData.get_all_kmers(self.k_mer)\n",
    "\n",
    "        number_dict = dict()\n",
    "        word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "        for i, w in enumerate(word_list):\n",
    "            word_dict[w] = i + 4\n",
    "            number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "\n",
    "        self.word_dict = word_dict\n",
    "        self.number_dict = number_dict\n",
    "\n",
    "        self.vocab_size = len(word_dict)\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.batch = []\n",
    "        positive = negative = 0\n",
    "        while positive != data_count / 2 or negative != data_count / 2:\n",
    "            is_positive = randrange(0, 2)\n",
    "\n",
    "            tokens_a_index, tokens_b_index = 0, 0\n",
    "            while tokens_a_index == tokens_b_index:\n",
    "                tokens_a_index, tokens_b_index = randrange(len(self.dna_nucleotides)), randrange(\n",
    "                    len(self.dna_nucleotides))\n",
    "\n",
    "            if is_positive:\n",
    "                dna_a = self.dna_nucleotides[tokens_a_index]\n",
    "                dna_b = dna_a\n",
    "            else:\n",
    "                dna_a = self.dna_nucleotides[tokens_a_index]\n",
    "                dna_b = self.dna_nucleotides[tokens_b_index]\n",
    "\n",
    "            rand_len = randrange(128, 256)\n",
    "\n",
    "            dna_a = dna_a[0:len(dna_a) // 2][0:rand_len]  # max_len//2 - 3]\n",
    "            dna_b = dna_b[len(dna_b) // 2:][0:rand_len]  # max_len//2 - 3]\n",
    "            tokens_a = tokenizer(dna_a, word_dict, k_mer, stride=1)\n",
    "            tokens_b = tokenizer(dna_b, word_dict, k_mer, stride=1)\n",
    "            input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "            segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "            # MASK LM\n",
    "            n_pred = min(max_mask_count, max(1, int(round(len(input_ids) * 0.15)))) // self.k_mer  # 15 % of tokens in one sentence\n",
    "            cand_masked_pos = [i for i, token in enumerate(input_ids)\n",
    "                               if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "\n",
    "            # remove N and gaps from cand_masked_pos\n",
    "            cand_masked_pos_copy = cand_masked_pos.copy()\n",
    "            for position in cand_masked_pos_copy:\n",
    "                remove_flag = False\n",
    "                for s in range(self.k_mer):\n",
    "                    if position + s < len(input_ids):\n",
    "                        key = self.number_dict[input_ids[position + s]]\n",
    "                        if (\"N\" in key) or (\"-\" in key):\n",
    "                            remove_flag = True\n",
    "                            break\n",
    "                if remove_flag:\n",
    "                    cand_masked_pos.remove(position)\n",
    "\n",
    "            shuffle(cand_masked_pos)\n",
    "\n",
    "            # if the position remains is less than 15%, mask them all\n",
    "            if len(cand_masked_pos) < n_pred:\n",
    "                n_pred = len(cand_masked_pos)\n",
    "\n",
    "            masked_tokens, masked_pos = [], []\n",
    "            for pos in cand_masked_pos[:n_pred]:\n",
    "                for s in range(self.k_mer):\n",
    "                    if pos + s < len(input_ids):\n",
    "                        masked_pos.append(pos + s)\n",
    "                        masked_tokens.append(input_ids[pos + s])\n",
    "                        input_ids[pos + s] = word_dict['[MASK]']  # make mask\n",
    "\n",
    "            # Zero Paddings\n",
    "            n_pad = max_len - len(input_ids)\n",
    "            input_ids.extend([0] * n_pad)\n",
    "            segment_ids.extend([0] * n_pad)\n",
    "\n",
    "            # Zero Padding (100% - 15%) tokens\n",
    "            if max_mask_count > len(masked_pos):\n",
    "                n_pad = max_mask_count - len(masked_pos)\n",
    "                masked_tokens.extend([0] * n_pad)\n",
    "                masked_pos.extend([0] * n_pad)\n",
    "\n",
    "            if is_positive and positive < data_count / 2:\n",
    "                self.batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])  # IsNext\n",
    "                positive += 1\n",
    "            elif not is_positive and negative < data_count / 2:\n",
    "                self.batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])  # NotNext\n",
    "                negative += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = torch.Tensor(self.batch[idx][0])\n",
    "        seg = torch.Tensor(self.batch[idx][1])\n",
    "        msk_tok = torch.Tensor(self.batch[idx][2])\n",
    "        msk_pos = torch.Tensor(self.batch[idx][3])\n",
    "        label = torch.Tensor([self.batch[idx][4]])\n",
    "\n",
    "        ids, seg, msk_pos = ids.type(torch.IntTensor), seg.type(torch.IntTensor), msk_pos.type(torch.int64)\n",
    "\n",
    "        msk_tok = msk_tok.type(torch.LongTensor)\n",
    "        label = label.type(torch.LongTensor)\n",
    "\n",
    "        return ids, seg, msk_pos, msk_tok, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEHIASapIyGz"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jnNFTO3xIzcO"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (142750074.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1610462/142750074.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pickleclass Embedding(nn.Module):\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, maxlen, n_segments, device):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(int(vocab_size), int(d_model))  # token embedding\n",
    "        self.pos_embed = nn.Embedding(int(maxlen), int(d_model))  # position embedding\n",
    "        self.seg_embed = nn.Embedding(int(n_segments), int(d_model))  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(int(d_model))\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=self.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, device):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    pad_attn_mask = pad_attn_mask.to(device)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, n_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_k)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)  # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)  # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.linear = nn.Linear(self.n_heads * self.d_v, self.d_model)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)  # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention(self.d_k)(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)  # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = self.linear(context)\n",
    "\n",
    "        return self.layernorm(output + residual), attn  # output: [batch_size x len_q x d_model]\n",
    "\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.l1 = nn.Linear(d_model, d_ff)\n",
    "        self.l2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.relu = GELU()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs\n",
    "        output = self.l1(inputs)\n",
    "        output = self.relu(output)\n",
    "        output = self.l2(output)\n",
    "        return self.layer_norm(output + residual)\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        self.d_k = d_k\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(\n",
    "            self.d_k)  # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9)  # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, maxlen, n_segments, n_layers, d_k, d_v, n_heads, device):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_model, maxlen, n_segments, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, d_k, d_v, n_heads) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = GELU()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "        self.device=device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            # embedding layer\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        # classifier pos/neg (it will be decided by first token(CLS))\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0]))  # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled)  # [batch_size, 2]\n",
    "\n",
    "        # classifier mask\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))  # [batch_size, max_pred, d_model]\n",
    "        # get masked position from final output of transformer.\n",
    "        h_masked = torch.gather(output, 1, masked_pos)  # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias  # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_clsf, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCsAwf11JJDi"
   },
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkKDUYSeWapx"
   },
   "source": [
    "Mount your drive to access the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKfdWluKJrMs",
    "outputId": "a0e8ebc7-48ae-401b-92e6-1131e5d3742a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQBtv4hBP7cc"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEshydwlBusq"
   },
   "outputs": [],
   "source": [
    "# File path of the input\n",
    "# input_path = \"/content/drive/MyDrive/BIOSCAN/full_training.tsv\"\n",
    "# input_path = \"/content/drive/MyDrive/BIOSCAN/medium_training.tsv\"\n",
    "input_path = \"/content/drive/MyDrive/BIOSCAN/small_training.tsv\"\n",
    "\n",
    "\n",
    "# dataloader to get a batch of data\n",
    "dataset = SampleDNAData(file_path=input_path, k_mer=4, data_count=256, max_mask_count=80, max_len=512)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# define the model\n",
    "# DNABert configuration\n",
    "config = {\n",
    "    \"d_model\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12 \n",
    "\n",
    "}\n",
    "# # first config\n",
    "# config = {\n",
    "#     \"d_model\": 16,\n",
    "#     \"n_heads\": 2,\n",
    "#     \"n_layers\": 5 \n",
    "\n",
    "# }\n",
    "model = BERT(dataset.vocab_size, config[\"d_model\"], dataset.max_len, 2, config[\"n_layers\"], 32, 32, config[\"n_heads\"], device=device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "UR3ajJsf4ZN3",
    "outputId": "3fe3b95c-f51a-4a4c-d846-566a913833c1"
   },
   "outputs": [],
   "source": [
    "saving_path = \"/content/drive/MyDrive/BIOSCAN/model_checkpoints/\"\n",
    "epoch_loss_list = []\n",
    "training_epoch = 1000\n",
    "continue_epoch = 0\n",
    "\n",
    "''' \n",
    "If you lost your connection and want to continue the training process, \n",
    "uncomment this part, load your last model, optimizer, and loss, \n",
    "choose the epoch you \n",
    "want to continue from\n",
    "'''\n",
    "# continue_epoch = 100\n",
    "# model.load_state_dict(torch.load(saving_path + f'model_{continue_epoch}.pth'))\n",
    "# optimizer.load_state_dict(torch.load(saving_path + f\"optimizer_{continue_epoch}.pth\"))\n",
    "\n",
    "# a_file = open(saving_path + \"loss.pkl\", \"rb\")\n",
    "# epoch_loss_list = pickle.load(a_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLAbcratJLHT",
    "outputId": "353df421-20ae-461d-f89c-5dcc4efa7fe2"
   },
   "outputs": [],
   "source": [
    "# strat training\n",
    "for epoch in range(continue_epoch, training_epoch + 1):\n",
    "    epoch_loss = 0\n",
    "    for ids, seg, msk_pos, masked_tokens, is_pos in dataloader:\n",
    "        ids = ids.to(device)\n",
    "        seg = seg.to(device)\n",
    "        msk_pos = msk_pos.to(device)\n",
    "        masked_tokens = masked_tokens.to(device)\n",
    "        is_pos = is_pos.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_lm, logits_clsf, outputs= model(ids, seg, msk_pos)\n",
    "\n",
    "        loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)  # for masked LM\n",
    "        loss_lm = (loss_lm.float()).mean()\n",
    "        loss_clsf = criterion(logits_clsf, torch.squeeze(is_pos))  # for sentence classification\n",
    "        loss = loss_lm + loss_clsf\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "\n",
    "    print(f\"epoch {epoch}: Loss is {epoch_loss}\")\n",
    "\n",
    "    # every 50 epoch save the checkpoints and save the loss in a list\n",
    "    if epoch % 50 == 0:\n",
    "        torch.save(model.state_dict(), saving_path + \"model_\" + str(epoch) +'.pth')\n",
    "        torch.save(optimizer.state_dict(), saving_path + \"optimizer_\" + str(epoch) +'.pth')\n",
    "\n",
    "        a_file = open(saving_path + \"loss.pkl\", \"wb\")\n",
    "        pickle.dump(epoch_loss_list, a_file)\n",
    "        a_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmfJg62-Ahp1"
   },
   "source": [
    "## Plot the loss after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "K4v-JAdIWzXg",
    "outputId": "dc412d50-6004-4ae9-be66-f4cd860e9116"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss_list, label =\"Epoch Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('/content/drive/MyDrive/BIOSCAN/model_checkpoints_new/loss_per_epoch.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxuQbqskaDkJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
